{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras 간단 기능 정리\n",
    "<b><h2>1) 전처리(Preprocessing) : 토큰화와 정수 인코딩</h2></b>\n",
    "\n",
    "- <b>Tokenizer()</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: [1, 2, 3, 4, 6, 7]\n",
      "word_index: {'the': 1, 'earth': 2, 'is': 3, 'an': 4, 'awesome': 5, 'place': 6, 'live': 7}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "t = Tokenizer()\n",
    "fit_text = \"The earth is an awesome place live\"\n",
    "t.fit_on_texts([fit_text])\n",
    "\n",
    "test_text = \"The earth is an great place live\"\n",
    "sequences = t.texts_to_sequences([test_text])[0]\n",
    "\n",
    "print(\"sequences:\", sequences)   # great는 단어 집합에 없으므로 출력되지 않음.\n",
    "print(\"word_index:\", t.word_index)   # 단어 집합 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenizer()를 정의해준다.\n",
    "2. input할 data를 준비하고 read한다.\n",
    "3. texts_to_sequences(texts) 는 텍스트의 각 텍스트를 일련의 정수 변환\n",
    "4. fit_on_texts 는 텍스트 목록을 기반으로 내부 어휘를 업데이트, 텍스트에 목록이 포함된 경우 목록의 각 항목의 토큰인 것으로 가정<br> \n",
    "  ***texts_to_sequences또는 texts_to_matrix 수행되어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>pad_sequence()</b><br>\n",
    "    훈련 데이터에서 샘플들의 길이가 서로 다를 수 있다. 모델의 입력으로 사용하려면 샘플의 길이를 동일하게 맞추어야 할 때가 있는데 이때, 패딩작업을 pad_sequence()로 한다. 0을 넣어서 길이가 다른 샘플들의 길이를 맞추거나 일부를 잘라서 길이를 맞춘다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [0, 7, 8]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "data = [[1, 2, 3], [3, 4, 5, 6], [7, 8]]\n",
    "pad_sequences(data, maxlen=3, padding='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 첫번째 인자 = 패딩을 진행할 데이터\n",
    "2. maxlen = 모든 데이터에 대해서 정규화 할 길이\n",
    "3. padding = 'pre'를 선택하면 앞에 0을 채우고 'post'를 선택하면 뒤에 0을 채움."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>2) 워드 임베딩(Word Embedding</h2></b>\n",
    "- 워드 임베딩이란? 텍스트 내의 단어들을 밀집 벡터로 만드는 것\n",
    "- 원-핫 벡터? 대부분의 0의 값을 가지고 단 하나의 1의 값을 가진 벡터<br>\n",
    "대부분의 값이 0인 이러한 벡터를 희소벡터라고 하며 단어 수만큼 벡터의 차원을 가지며 단어 간 유사도가 모두 동일한 단점.\n",
    "- 밀집 벡터란? 표기상, 의미상 반대 벡터가 존재하며 대부분의 값이 실수이고 저차원인 밀집 벡터<br>\n",
    "예) [0.1 -1.2 0.8 0.2 1.8] # 상대적으로 저차원이며 실수값을 가짐\n",
    "- 단어를 밀집 벡터로 만드는 작업을 워드 임베딩이라고 함.\n",
    "- 초기 값은 랜덤값을 가지지만 학습되는 방법과 같은 방식으로 값이 변경됨\n",
    "\n",
    "- <b>Embedding()</b><br>\n",
    "밀집 벡터로 만드는 역할<br>\n",
    "정수 인코딩이 된 단어들을 입력 받음<br>\n",
    "(number of samples, input_length)인 2D 정수 텐서를 입력\n",
    "(number of samples, input_length, embedding word dimentionality)인 3D 텐서를 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 토큰화와 단어 토큰화\n",
    "text=[['Hope', 'to', 'see', 'you', 'soon'],['Nice', 'to', 'see', 'you', 'again']]\n",
    "\n",
    "# 각 단어에 대한 정수 인코딩\n",
    "text=[[0, 1, 2, 3, 4],[5, 1, 2, 3, 6]]\n",
    "\n",
    "# 위 데이터가 아래의 임베딩 층의 입력이 된다.\n",
    "Embedding(7, 2, input_length=5)\n",
    "# 7은 단어의 개수. 즉, 단어 집합(vocabulary)의 크기이다.\n",
    "# 2는 임베딩한 후의 벡터의 크기이다.\n",
    "# 5는 각 입력 시퀀스의 길이. 즉, input_length이다.\n",
    "\n",
    "# 각 정수는 아래의 테이블의 인덱스로 사용되며 Embeddig()은 \n",
    "# 각 단어에 대해 임베딩 벡터를 리턴한다.\n",
    "+------------+------------+\n",
    "|   index    | embedding  |\n",
    "+------------+------------+\n",
    "|     0      | [1.2, 3.1] |\n",
    "|     1      | [0.1, 4.2] |\n",
    "|     2      | [1.0, 3.1] |\n",
    "|     3      | [0.3, 2.1] |\n",
    "|     4      | [2.2, 1.4] |\n",
    "|     5      | [0.7, 1.7] |\n",
    "|     6      | [4.1, 2.0] |\n",
    "+------------+------------+\n",
    "# 위의 표는 임베딩 벡터가 된 결과를 예로서 정리한 것이고 \n",
    "# Embedding()의 출력인 3D 텐서를 보여주는 것이 아님."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 첫번째 인자 = 단어 집합의 크기. 즉, 총 단어의 개수\n",
    "2. 두번째 인자 = 임베딩 벡터의 출력 차원. 결과로서 나오는 임베딩 벡터의 크기\n",
    "3. input_length = 입력 시퀀스의 길이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>3) 모델링(Modeling)</h2></b>\n",
    "- <b>Sequential()</b><br>\n",
    "입력층, 은닉층, 출력층을 구성하기 위해 Sequential()을 사용<br>\n",
    "Sequential()을 model로 선언한 뒤에 model.add()라는 코드를 통해 층을 단계적으로 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary, output_dim, input_length))  # 층 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Dense()</b><br>\n",
    "전결합층(fully-conntected layer)을 추가<br>\n",
    "model.add()를 통해 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense,(1, input_dim=3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 첫번째 인자 = 출력 뉴런의 수.\n",
    "2. input_dim = 입력 뉴런의 수. (입력의 차원)\n",
    "3. activation = 활성화 함수\n",
    "    - linear : 디폴트 값으로 별도 활성화 함수 없이 입력 뉴런과 가중치의 계산 결과 그대로 출력. Ex) 선형 회귀\n",
    "    - sigmoid : 시그모이드 함수. 이진 분류 문제에서 출력층에 주로 사용되는 활성화 함수.\n",
    "    - softmax : 소프트맥스 함수. 셋 이상을 분류하는 다중 클래스 분류 문제에서 출력층에 주로 사용되는 활성화 함수.\n",
    "    - relu : 렐루 함수. 은닉층에 주로 사용되는 활성화 함수.\n",
    "\n",
    "Dense()를 사용하여 전결합층을 하나 더 추가 (input=4, hiddle layer=8, output=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # 출력층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>summary()</b><br>\n",
    "모델의 정보를 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 49\n",
      "Trainable params: 49\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # 출력층\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>4. 컴파일(Compile)과 훈련(Training)</h2></b>\n",
    "- <b>compile()</b><br>\n",
    "모델을 기계가 이해할 수 있도록 컴파일<br>\n",
    "오차 함수와 최적화 방법, 메트릭 함수를 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "max_features = 10000\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))  # 밀집벡터 생성\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. optimizer : 훈련 과정을 설정하는 옵티마이저를 설정합니다. 'adam'이나 'sgd'와 같이 문자열로 지정\n",
    "2. loss : 훈련 과정에서 사용할 손실 함수(loss function)를 설정<br>\n",
    "예) mean_squared_error(회귀), categorical_crossentropy(다중클래스 분류), binary_crossentropy(이진 분류)<br>\n",
    "3. metrics : 훈련을 모니터링하기 위한 지표를 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>fit()</b><br>\n",
    "모델을 학습<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, validation_data(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 첫번째 인자 = 훈련 데이터에 해당\n",
    "2. 두번째 인자 = 지도 학습에서 레이블 데이터에 해당\n",
    "3. epochs = 에포크. 에포크 1은 전체 데이터를 한 차례 훑고 지나갔음을 의미함. 정수값 기재 필요. 총 훈련 횟수를 정의\n",
    "4. batch_size = 배치 크기. 기본값은 32. 미니 배치 경사 하강법을 사용하고 싶지 않을 경우에는 batch_size=None을 기재\n",
    "5. verbose = 학습 중 출력되는 문구를 설정\n",
    "    - 0 : 아무 것도 출력하지 않습니다.\n",
    "    - 1 : 훈련의 진행도를 보여주는 진행 막대를 보여줍니다.\n",
    "    - 2 : 미니 배치마다 손실 정보를 출력합니다.\n",
    "6. validation_data(x_val, y_val) = 검증 데이터(validation data)를 사용\n",
    "7. validation_split = validation_data 대신 사용, validation_split=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#verbose = 1일 경우.<br>\n",
    "Epoch 88/100<br>\n",
    "7/7 [==============================] - 0s 143us/step - loss: 0.1029 - acc: 1.0000\n",
    "\n",
    "#verbose = 2일 경우.<br>\n",
    "Epoch 88/100<br>\n",
    " -0s - loss: 0.1475 - acc: 1.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>5. 평가(Evaluation)와 예측(Prediction)</h2></b>\n",
    "- <b>evaluate()</b><br>\n",
    "테스트 데이터를 통해 학습한 모델에 대한 정확도를 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 첫번째 인자 = 테스트 데이터에 해당\n",
    "2. 두번째 인자 = 지도 학습에서 레이블 테스트 데이터에 해당\n",
    "3. batch_size = 배치 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>predict()</b><br>\n",
    "임의의 입력에 대한 모델의 출력값을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_input, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 첫번째 인자 = 예측하고자 하는 데이터.\n",
    "2. batch_size = 배치 크기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>6. 모델의 저장(Save)과 로드(Load)</h2></b>\n",
    "- <b>save()</b><br>\n",
    "인공 신경망 모델을 hdf5 파일에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_name.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>load_model()</b><br>\n",
    "저장해둔 모델을 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"model_name.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
