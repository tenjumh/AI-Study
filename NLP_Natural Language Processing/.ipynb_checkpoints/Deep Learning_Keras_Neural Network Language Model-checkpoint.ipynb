{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 피드 포워드 신경망 언어 모델(Neural Network Language Model, NNLM)\n",
    "\n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/DL/16F08CE0-73DE-4EC2-BA6E-F9564EDF29B9.jpeg?raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>3. 피드 포워드 신경망 언어 모델(NNLM)</b></h2><br>\n",
    "- 예문 : \"what will the fat cat sit on\"\n",
    "\n",
    "1. One-Hot Enconding을 한다\n",
    "\n",
    "```C\n",
    "what = [1, 0, 0, 0, 0, 0, 0]\n",
    "will = [0, 1, 0, 0, 0, 0, 0]\n",
    "the = [0, 0, 1, 0, 0, 0, 0]\n",
    "fat = [0, 0, 0, 1, 0, 0, 0]\n",
    "cat = [0, 0, 0, 0, 1, 0, 0]\n",
    "sit = [0, 0, 0, 0, 0, 1, 0]\n",
    "on = [0, 0, 0, 0, 0, 0, 1]\n",
    "```\n",
    "2. NNLM은 n-gram언어 모델과 유사 : 정해진 n개의 단어만 참고\n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/DL/NNLM_1.PNG?raw=True)\n",
    "\n",
    "    - 총 4개의 층으로 이뤄진 신경망<br>\n",
    "    1) input layer : 정해진 n 단어의 인코딩 값이 입력<br>\n",
    "    2) prejection layer(투사층) : 활성화 함수는 존재하지 않음<br>\n",
    "        -투사층의 크기를 M으로 설정하면 V x M 크기의 가중치 행렬 곱<br>\n",
    "        -V: 단어 집합의 크기(원-핫 벡터의 차원)<br>\n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/DL/NNLM_2.PNG?raw=True)\n",
    "\n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/DL/NNLM_3.PNG?raw=True)\n",
    "    - x는 각 단어의 원-핫 벡터, 예측하고자 하는 단어가 문장에서 t번째 단어\n",
    "    - 윈도우의 크기 n, 룩업 테이블을 의미하는 함수 lookup, (;)은 연결 기호\n",
    "    - 투사층 식\n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/DL/NNLM_4.PNG?raw=True)\n",
    "    - 다음 층부터는 일방 신경망 은닉층 사용 (활성화 함수 존재)\n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/DL/NNLM_5.PNG?raw=True)\n",
    "    - 은닉층 식\n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/DL/NNLM_6.PNG?raw=True)\n",
    "    - 출력층 식\n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/DL/NNLM_7.PNG?raw=True)\n",
    "\n",
    "3. NNLM의 이점과 한계\n",
    "    - 개선점 : 밀집 벡터(벡터의 원소이 실수값을 가지며 원핫벡터보다 저차원을 가지는 벡터)를 사용하여 단어의 유사도를 표현, 희소문제(원소의 대부분이 0을 가짐) 해결\n",
    "    - 한계점 : 정해진 길이의 입력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
