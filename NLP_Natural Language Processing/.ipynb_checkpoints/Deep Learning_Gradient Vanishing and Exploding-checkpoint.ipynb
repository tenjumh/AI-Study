{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기울기 소실(Gradient Vanishing)과 폭주(Exploding)\n",
    "<b>기울기 소실(Gradient Vanishing)이란?</b>\n",
    "- 역전파 과정에서 입력층으로 갈 수록 기울기(Gradient)가 점차적으로 작아지는 현상이 발생\n",
    "- 입력층에 가까운 층들에서 가중치들이 업데이트가 제대로 되지 않으면 결국 최적의 모델을 찾을 수 없게 됨\n",
    "\n",
    "<b>기울기 폭주(Gradient Exploding)이란?</b>\n",
    "- 기울기가 점차 커지더니 가중치들이 비정상적으로 큰 값이 되면서 결국 발산\n",
    "- 순환 신경망(Recurrent Neural Network, RNN)에서 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1. ReLU와 ReLU의 변형들</b><br>\n",
    "    - 시그모이드 함수를 사용하면 입력의 절대값이 클 경우에 시그모이드 함수의 출력값이 0 또는 1에 수렴하면서 기울기가 0에 가까워짐\n",
    "    - ReLU나 ReLU의 변형 함수와 같은 Leaky ReLU를 사용하면 어느정도 해결됨\n",
    "\n",
    "<b>2. 그래디언트 클리핑(Gradient Clipping)</b><br>\n",
    "    - 기울기 폭주를 막기 위해 기울기 값을 자르는 것을 의미\n",
    "    - 케라스 예시\n",
    "    from tensorflow.keras import optimizers\n",
    "    Adam = optimizers.Adam(lr=0.0001, clipnorm=1.)\n",
    "    \n",
    "<b>3. 가중치 초기화(Weight initialization)</b><br>\n",
    "    초기 가중치를 어떤값을 주느냐에 따라 결과 같이 달라진다.<br>\n",
    "    - 평균이 0, 표준편차 0.01을 따르는 Gaussian분포에 의해 초기 가중치를 주면\n",
    "        -> shallow network에서는 잘 작동되지만 deep network에서는 작동이 안됨 \n",
    "        -> 10개의 Layer를 가지는 network에서 층이 깊어질수록 weight들이 곱해지므로 평균은 0에 수렴하지만, 표준편차마저 점점 줄어들어 0으로 수렴 \n",
    "        -> back propogation에서 업데이트 할 때, Input 값들이 애초에 너무 작아서, gradient가 매우 작아 Weight들의 업데이트가 거의 발생하지 않음 \n",
    "    - 평균이 0, 표준편차 1을 따르는 Gaussian분포에 의해 초기 가중치를 주면\n",
    "        -> Weight 값들이 1아니면 -1로 매우 치우쳐지게 되며 Gradient가 0으로 수렴하게 되고, update가 발생하지 않는 문제가 발생\n",
    "\n",
    "   <b>1) 세이비어 초기화(Xavier Initialization)</b><br>\n",
    "   균등 분포(Uniform Distribution) 또는 정규 분포(Normal distribution)로 초기화\n",
    "   (이전 층의 뉴런의 개수를 n(in), 다음 층의 뉴런의 개수를 n(out))\n",
    "   - 균등분포범위\n",
    "![세이비어초기화_균등](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/DL/%EA%B0%80%EC%A4%91%EC%B9%98%EC%B4%88%EA%B8%B0%ED%99%94_%EA%B7%A0%EB%93%B1.PNG?raw=True)\n",
    "   - 정규분포범위\n",
    "![세이비어초기화_정규](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/DL/%EA%B0%80%EC%A4%91%EC%B9%98%EC%B4%88%EA%B8%B0%ED%99%94_%EC%A0%95%EA%B7%9C.PNG?raw=True)\n",
    "   - 여러 층의 기울기 분산 사이에 균형을 맞춰서 특정 층이 너무 주목 받거나 다른 층에 뒤쳐지는 것을 방지\n",
    "   - 단 S자 형태 활성함수에는 좋은 성능을 보임.<br>\n",
    "       \n",
    "   <b>2) He 초기화(He Initialization)</b><br>\n",
    "   균등 분포(Uniform Distribution) 또는 정규 분포(Normal distribution)로 초기화<br>\n",
    "   다음 층의 뉴런의 수를 반영하지 않음   <br> \n",
    "   (이전 층의 뉴런의 개수를 n(in))<br>\n",
    "   - 균등분포범위\n",
    "![He초기화_균등](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/DL/%EA%B0%80%EC%A4%91%EC%B9%98%EC%B4%88%EA%B8%B0%ED%99%94_He%EA%B7%A0%EB%93%B1.PNG?raw=True)\n",
    "   - 정규분포범위\n",
    "![He초기화_정규](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/DL/%EA%B0%80%EC%A4%91%EC%B9%98%EC%B4%88%EA%B8%B0%ED%99%94_He%EC%A0%95%EA%B7%9C.PNG?raw=True)\n",
    "   - 여러 층의 기울기 분산 사이에 균형을 맞춰서 특정 층이 너무 주목 받거나 다른 층에 뒤쳐지는 것을 방지\n",
    "   - 단 ReLU 계열 함수에는 좋은 성능을 보임.\n",
    "\n",
    "<b>4. 배치 정규화(Batch Normalization)</b><br>\n",
    "    - 가중치 초기화도 훈련 중 기울기 소실 또는 폭주가 다시 일어날 수 있어 배치 정규화를 해준다.\n",
    "    - 각 층에 들어가는 입력을 평균과 분산으로 정규화\n",
    "![푸리에 변환](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/DL/batch_normal_2.PNG?raw=True)\n",
    "    - 배치 정규화의 수식\n",
    "![푸리에 변환](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/DL/batch_normal.PNG?raw=True)\n",
    "    - 학습 시 배치 단위의 평균과 분산들을 차례대로 받아 이동 평균과 이동 분산을 저장해 놓았다가 테스트 할 때는 해당 배치의 평균과 분산을 구하지 않고 구해놓은 평균과 분산으로 정규화\n",
    "    - 단점은 모델을 복잡하게 하면 추가 계산으로 속도가 느려짐.\n",
    "    - 미니 배치 크기에 의존적 : 너무 작은 배치 크기에서는 잘 동작하지 않음.\n",
    "    - RNN에 적용하기 어렵다\n",
    "\n",
    "<b>5. 층 정규화(Layer Normalization)</b><br>\n",
    "배치 정규화와 차이점을 시각화\n",
    "![푸리에 변환](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/DL/Layer_nomal.PNG?raw=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
