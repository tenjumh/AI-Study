{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *표제어추출과 어간추출 차이\n",
    "- 표제어 추출 : 문맥을 고려하여 수행했을 대의 결과는 해당 단어의 품사 정보를 보존(POS태그 보존)\n",
    "- 어간 추출 : 품사 정보가 보존되지 않음(POS태그 보전 X) - 사전에 존재하지 않은 단어가 많음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 표제어 추출(Lemmatization)\n",
    "- 표제어 추출은 단어들로부터 표제어를 찾아가는 과정<br>\n",
    "- 단어들이 다른 형태를 갖더라도 그 뿌릴 단어를 찾어가서 단어의 개수를 줄일 수 있다는 판단<br>\n",
    "  예)am, are, is는 다른 스펠링이지만 그 뿌리는 be임. -> 표제어는 \"be\"<br>\n",
    "- 형태소 : 의미를 가진 가장 작은 단위\n",
    "  1)어간-단어의 의미를 담고 있는 단어의 핵심 부분\n",
    "  2)접사-단어에 추가적인 의미를 주는 부분\n",
    "  예) cats - cat(어간), s(접사)\n",
    "- 형태학 : 형태소부터 단어들을 만들어가는 학문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fxk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "# NLTK에서는 표제어추출 도구로 \"WordNetLemmatizer\"\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "n = WordNetLemmatizer()\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([n.lemmatize(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNetLemmatizer는 입력으로 단어가 동사 품사라는 사실을 알려줌\n",
    "위의 \"dies\", \"watched\", \"has\"를 동사로 쓰였다고 알려주면 표제어 추출기는 정확한 Lemma를 추출함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('dies', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('watched', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('has', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 어간 추출(Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = PorterStemmer()\n",
    "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['formal', 'allow', 'electric']\n"
     ]
    }
   ],
   "source": [
    "words = ['formalize', 'allowance', 'electricical']\n",
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['formal', 'allow', 'electric']\n"
     ]
    }
   ],
   "source": [
    "words = ['formalize', 'allowance', 'electricical']\n",
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 포터 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "s = PorterStemmer()\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 래커스터 스태머 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "l = LancasterStemmer()\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([l.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 한국어에서의 어간 추출\n",
    "- 5언 9품사 : 체언(명사, 대명사), 수식언(관형사, 부사), 관계언(조사), 독립언(감탄사), 용언(동사, 형용사)<br>\n",
    "<br>\n",
    "<b>(1) 활용</b><br>\n",
    "- 활용 : 용언의 어간(stem)이 어미(ending)를 가지는 일<br>\n",
    " 1)어간 : 용언을 활용할 때, 원칙적으로 모양이 변하지 않는 부분<br>\n",
    "활용에서 어미에 선행하는 부분, 때론 어간의 모양도 바뀔 수 있음<br>\n",
    "예) 긋다, 긋고, 그어서, 그어라<br>\n",
    " 2)어미 : 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분, 여러 문법적 기능을 수행 <br>\n",
    " *어간의 모습이 일정하면 규칙활용, 변하면 불규칙활용 <br>\n",
    " <br>\n",
    "<b>(2) 규칙활용</b><br>\n",
    "예) '잡/어간 + 다/어미'<br>\n",
    "-> 어간이 어미가 붙기전의 모습과 어미가 붙은 후의 모습이 동일<br>\n",
    "<br>\n",
    "<b>(3) 불규칙활용</b><br>\n",
    "예) ‘듣-, 돕-, 곱-, 잇-, 오르-, 노랗-’ 등이 ‘듣/들-, 돕/도우-, 곱/고우-, 잇/이-, 올/올-, 노랗/노라-’<br>\n",
    "-> 어간의 형식이 달라짐<br>\n",
    "예) ‘오르+ 아/어→올라, 하+아/어→하여, 이르+아/어→이르러, 푸르+아/어→푸르러’<br>\n",
    "-> 일반적인 어미가 아닌 특수한 어미를 취하는 경우<br>\n",
    "<br>\n",
    "*다양한 불규칙 활용의 예<br>\n",
    "https://namu.wiki/w/한국어/불규칙%20활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
