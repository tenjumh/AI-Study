{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "- MSE(Mean Squared Error) ![푸리에 변환](https://wikidocs.net/images/page/24987/mse.PNG?raw=True)<br>\n",
    "오차 제곱 평균을 의미<br>\n",
    "연속형 변수를 예측 사용<br>\n",
    "\n",
    "- Cross-Entropy  ![푸리에 변환](https://wikidocs.net/images/page/24987/%ED%81%AC%EB%A1%9C%EC%8A%A4%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC.PNG?raw=True)<br>\n",
    "y:실제값(0 or 1), ^y:예측값(0~1, 확률)<br>\n",
    "낮은 확률로 예측해서 맞추거나 높은 확률로 예측해서 틀릴 경우 Loss가 큼<br>\n",
    "이진 분류의 경우 : binary crossentropy 사용\n",
    "다중 클래스 분류 : categorical crossentropy 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 옵티마이저(Optimizer)\n",
    "![푸리에 변환](https://wikidocs.net/images/page/36033/%EC%97%AD%EC%A0%84%ED%8C%8C_%EA%B3%BC%EC%A0%95.PNG?raw=True)\n",
    "- 손실함수의 값을 줄여 나갈 때 사용함<br>\n",
    "\n",
    "<b>1) 배치 경사 하강법(BGD)</b><br>\n",
    "    - loss를 구할 때 전체 데이터를 고려\n",
    "    - 머신러닝에서 1번의 훈련횟수를 1epoch라고 하는데 BGD는 1epoch에 모든 매개변수를 업데이트\n",
    "    - 전체 데이터를 고려하기에 시간이 오래걸리고 메모리를 크게 요구\n",
    "    - 글로벌 미니멈을 찾을 수 있는 장점\n",
    "    - 케라스 예시<br>\n",
    "    model.fit(X_train, y_train, batch_size=len(trainX))<br>\n",
    "<br>\n",
    "\n",
    "<b>2) 확률적 경사 하강법(SGD)</b><br>\n",
    "    - 매개변수 값을 조정 시 전체 데이터가 아닌 랜덤으로 선택한 하나의 데이터만 계산\n",
    "    - 케라스 예시<br>\n",
    "    model.fit(X_train, y_train, batch_size=1)<br>\n",
    "![d](https://wikidocs.net/images/page/24987/%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95SGD.PNG?raw=True)\n",
    "<br>\n",
    "\n",
    "<b>3) 미니 배치 경사 하강법</b><br>\n",
    "    - 전체 데이터도 아니고, 1개의 데이터도 아니고 정해진 양에 대해서만 계산하여 매개 변수의 값을 조정\n",
    "    - 케라스 예시<br>\n",
    "    model.fit(X_train, y_train, batch_size=32)<br>\n",
    "\n",
    "<b>4) 모멤텀(Momentum)</b><br>\n",
    "    - 모멘텀(Momentum)은 관성이라는 물리학의 법칙을 응용한 방법\n",
    "    - 모멘텀 SGD는 경사 하강법에 관성을 더 해줌\n",
    "    - SGD에서 계산된 접선의 기울기에 한 시점(step) 전의 접선의 기울기값을 일정한 비율만큼 반영\n",
    "    - 마치 언덕에서 공이 내려올 때, 중간에 작은 웅덩이에 빠지더라도 관성의 힘으로 넘어서는 효과\n",
    "    - 로컬 미니멈에 도달하였을 때, 기울기가 0이라서 기존의 경사 하강법이라면 이를 글로벌 미니멈으로 잘못 인식하여 계산이 끝났을 상황이라도 모멘텀. 즉, 관성의 힘을 빌리면 값이 조절되면서 로컬 미니멈에서 탈출하는 효과\n",
    "    - 모멤텀 설명\n",
    "http://localhost:8888/notebooks/PycharmProjects/tenjumh/DeepLearning/Class%20tasks/Momentum_BatchNormal_DropOut_%EC%86%90%EA%B3%84%EC%82%B0.ipynb\n",
    "   http://localhost:8888/notebooks/PycharmProjects/tenjumh/DeepLearning/DeepLearning_Momentum%2C%20BatchNormal.%2C%20Dropout.ipynb\n",
    "    - 케라스 예시<br>\n",
    "    keras.optimizers.SGD(lr = 0.01, momentum= 0.9)<br>\n",
    "<br>\n",
    "\n",
    "<b>5) 아다그라드(Adagrad)</b><br>\n",
    "    - 각 매개변수에 서로 다른 학습률을 적용<br>\n",
    "    - 변화가 많은 매개변수는 학습률이 작게 설정되고 변화가 적은 매개변수는 학습률을 높게 설정<br>\n",
    "    - 케라스 예시<br>\n",
    "    keras.optimizers.Adagrad(lr=0.01, epsilon=1e-6)<br>\n",
    "    <br>\n",
    "    \n",
    "<b>6) 알엠에스프롭(RMSprop)</b><br>\n",
    "    - 아다그라드는 학습을 계속 진행한 경우에는, 나중에 가서는 학습률이 지나치게 떨어진다는 단점\n",
    "    - 이를 다른 수식으로 대체\n",
    "    - 케라스 예시<br>\n",
    "    keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)<br>\n",
    "<br>\n",
    "    \n",
    "<b>7) 아담(Adam)</b><br>\n",
    "    - 알엠에스프롭과 모멘텀 두 가지를 합친 듯한 방법\n",
    "    - 케라스 예시<br>\n",
    "    keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)<br>\n",
    "\n",
    "# 옵티마이즈 사용법\n",
    "https://keras.io/optimizers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 에포크, 배치크기와 이터레이션\n",
    "![푸리에 변환](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/ML/%EC%97%90%ED%8F%AC%EA%B7%B8_%EB%B0%B0%EC%B9%98_%EC%9D%B4%EB%8D%94%EB%A0%88%EC%9D%B4%EC%85%98.PNG?raw=True)<br>\n",
    "<b>1) 에포크(Epoch)</b><br>\n",
    "    - 전체 데이터에 대해서 순전파와 역전파가 끝난 상태\n",
    "    - 전체 데이터를 하나의 문제지에 비유한다면 문제지의 모든 문제를 끝까지 다 풀고, 정답지로 채점을 하여 문제지에 대한 공부를 한 번 끝낸 상태\n",
    "    - 에포크가 50이라고 하면, 전체 데이터 단위로는 총 50번 학습\n",
    "    \n",
    "    \n",
    "<b>2) 배치 크기(Batch size)</b><br>\n",
    "    - 몇 개의 데이터 단위로 매개변수를 업데이트 하는지\n",
    "    - 문제지에서 몇 개씩 문제를 풀고나서 정답지를 확인하느냐\n",
    "    - 2,000 문제가 수록되어있는 문제지의 문제를 200개 단위로 풀고 채점한다고 하면 이때 배치 크기는 200\n",
    "\n",
    "<b>3) 이터레이션(Iteration)</b><br>\n",
    "    - 한 번의 에포크를 끝내기 위해서 필요한 배치의 수\n",
    "    - 전체 데이터가 2,000일 때 배치 크기를 200으로 한다면 이터레이션의 수는 총 10개"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
