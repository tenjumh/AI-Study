{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)\n",
    "- 토픽 모델링 : 문서의 집합에서 토픽을 찾아내는 프로세스\n",
    "- 검색 엔진, 고객 민원 시스템 등과 같이 문서의 주제를 알아내는 일이 중요한 곳에서 사용\n",
    "- LDA : 문서들은 토픽들의 혼합으로 구성, 토픽들은 확률 분포에 기반하여 단어들을 생성한다 가정\n",
    "- LDA는 문서가 생성되던 과정을 역추적\n",
    "- 참고링크: https://lettier.com/projects/lda-topic-modeling/\n",
    "  -> 코드 작성 없이 입력한 문서들로부터 DTM을 만들고 LDA를 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1. 잠재 디리클레 할당</b><br>\n",
    "- 예) 3개의 문서가 있다고 하고 간단해서 눈으로도 토픽을 모델링할 수 있지만 수십만개 이상의 문서가 있는 경우는 어렵기에 LDA도움이 필요<br><br>\n",
    "문서1 : 저는 사과랑 바나나를 먹어요<br>\n",
    "문서2 : 우리는 귀여운 강아지가 좋아요<br>\n",
    "문서3 : 저의 깜찍하고 귀여운 강아지가 바나나를 먹어요<br><br>\n",
    "- 사용자는 매개변수(하이퍼파라미터) - 토픽 개수(k) 지정<br>\n",
    "- LDA가 위의 세 문서로부터 2개의 토픽을 찾은 결과 (각 문서의 토픽 분포와 각 토픽 내의 단어 분포를 추정)<br>\n",
    "\n",
    "<b><각 문서의 토픽 분포></b><br>\n",
    "문서1 : 토픽 A 100%<br>\n",
    "문서2 : 토픽 B 100%<br>\n",
    "문서3 : 토픽 B 60%, 토픽 A 40%<br>\n",
    "    \n",
    "<b><각 토픽의 단어 분포></b><br>\n",
    "토픽A : 사과 20%, 바나나 40%, 먹어요 40%, 귀여운 0%, 강아지 0%, 깜찍하고 0%, 좋아요 0%<br>\n",
    "토픽B : 사과 0%, 바나나 0%, 먹어요 0%, 귀여운 33%, 강아지 33%, 깜찍하고 16%, 좋아요 16%<br>\n",
    "    \n",
    "- LDA는 토픽의 제목을 정해주지 않지만, 2개의 토픽은 1) 과일에 대한 토픽과 2) 강아지에 대한 토픽"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. LDA의 가정</b><br>\n",
    "- LDA는 빈도수 기반의 표현 방법인 BoW의 행령 DTM 또는 TF-IDF 행렬을 입력, 즉 단어의 순서에 신경쓰지 않음<br>\n",
    "- LDA는 문서들 토픽을 뽑아내기 위해서 하기의 가정을 염두<br><br>\n",
    "    1) 문서에 사용할 단어의 개수 N 정함 \n",
    "        - ex) 5개의 단어를 정한다.<br>\n",
    "    2) 문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정 \n",
    "        - ex) 토픽이 2개라고 가정하였을 때 강아지 토픽을 60%, 과일 토픽을 40%와 같이 선택<br>\n",
    "    3) 문서에 사용할 각 단어를 정함<br>\n",
    "    3-1) 토픽 분포에서 토픽 T를 확률적으로 고름 \n",
    "        - ex) 60%확률로 강아지 토픽 선택, 40%확률로 과일 토픽 선택<br>\n",
    "    3-2) 선택한 토픽 T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어 고름 \n",
    "        - ex) 강아지 토픽을 선택하면 33%확률로 강아지란 단어를 선택<br>\n",
    "    4) 3)을 반복하면서 문서를 완성<br><br>\n",
    "- 이러한 과정을 통해 문서가 작성되었다는 가정하에 LDA는 토픽을 뽑아내기 위하 역공학(reverse engneering)을 수행<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3. LDA의 수행</b><br>\n",
    "\n",
    "<b>1) 사용자는 알고리즘에게 토픽의 개수 k를 알려줌</b>\n",
    "    - 하이퍼파라미터로 사용자가 토픽의 개수를 지정해주면 k개의 토픽이 M개의 전체 문서에 걸쳐 분포되어 있다고 가정\n",
    "<b>2) 모든 단어를 k개 중 하나의 토픽에 할당</b>\n",
    "    - 모든 문서의 모든 단어에 대해서 k개 중 하나의 토픽을 랜덤하게 할당\n",
    "    - 이 작업이 끝나면 각 문서는 토픽을 가지며, 토픽은 단어 분포를 가지는 상태\n",
    "    - 랜덤으로 할당하기에 전부 틀린 상태이며 한 단어가 한 문서에서 2회 등장하면 각 단어는 서로 다른 토픽에 할당될 수 있음 ????무슨말 ㅋㅋㅋ\n",
    "<b>3) 이제 모든 문서의 모든 단어에 대해서 아래의 사항을 반복 진행합니다. (iterative)</b>\n",
    "\n",
    "<b>3-1) 어떤 문서의 각 단어 w는 자신은 잘못된 토픽에 할당되어 있지만 다른 단어는 전부 올바른 토픽에 할당되어 있다고 가정, 이에 따라 단어 w는 아래의 두 가지 기준에 따라서 토픽이 재할당됨</b><br>\n",
    "    \n",
    "    - p(topic t | document d): 문서 d의 단어들 중 토픽 t에 해당하는 단어들의 비율<br>\n",
    "    - p(word w | topic t): 단어 w를 갖고 있는 모든 문서들 중 토픽 t가 할당된 비율<br>\n",
    "    \n",
    "    - 이를 반복하면 모든 할당이 완료된 수렴 상태가 됨<br>\n",
    "    - 예를 들어<br>\n",
    "    \n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/doc1.PNG?raw=True)\n",
    "\n",
    "- 위의 그림은 두 개의 문서 doc1과 doc2를 보여준다. 여기서 doc1의 세번째 단어 apple의 토픽을 결정한다.<br>\n",
    "\n",
    "![doc2](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/doc2.PNG?raw=True)\n",
    "\n",
    "- 우선 첫번째로 사용하는 기준은 문서 doc1의 단어들이 어떤 토픽에 해당하는지를 본다.<br>\n",
    "- doc1의 모든 단어들은 토픽 A와 토픽 B에 50대 50 비율로 할당되어져 있어, apple은 어디에도 속할 가능성이 있다.<br>\n",
    "\n",
    "![doc3](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/doc3.PNG?raw=True)\n",
    "\n",
    "- 두번째 기준은 단어 apple이 전체 문서에서 어떤 토픽에 할당되어져 있는지 본다. <br>\n",
    "- 이 기준에 따르면 단어 apple은 토픽 B에 할당될 가능성이 높다.<br><br>\n",
    "\n",
    "- 이 두가지 기준을 참고하여 LDA는 doc1의 apple을 어떤 토픽에 할당할지 결정한다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>4. 잠재 디리클레 할당과 잠재 의미 분석의 차이</b><br>\n",
    "- LSA : DTM을 차원 축소하여 축소 차원에서 근접 단어들을 토픽으로 묶음\n",
    "- LDA : 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽을 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>5. 실습을 통한 이해</b><br>\n",
    "- LDA는 gensim 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 정수 인코딩과 단어 집합 만들기\n",
    "- LSA챕터에서 사용한 Twenty Newsgroups 데이터 및 전처리 과정을 활용\n",
    "- tokenized_doc으로 저장한 상태부터 시작함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers','footeers','quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)\n",
    "print(dataset.target_names)\n",
    "\n",
    "# 텍스트 전처리\n",
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\",\" \")\n",
    "# 길이가 3이하인 단어 제거\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
    "\n",
    "# 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "# NLTK로부터 불용어 받아옴\n",
    "stop_words = stopwords.words('english')\n",
    "# 토큰화\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "# 불용어 제거\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [well, sure, story, seem, biased, disagree, st...\n",
       "1    [yeah, expect, people, read, actually, accept,...\n",
       "2    [although, realize, principle, strongest, poin...\n",
       "3    [notwithstanding, legitimate, fuss, proposal, ...\n",
       "4    [well, change, scoring, playoff, pool, unfortu...\n",
       "Name: clean_doc, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련용 뉴스 5개만 출력\n",
    "tokenized_doc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 단어에 정수 인코딩을 하는 동시에 각 뉴스에서의 단어 빈도수를 기록\n",
    "- 각 단어를 (word_id, word_frequency)의 형태로 바꿈\n",
    "- word_id는 단어가 정수 인코딩된 값이고 word_frequency는 해당 뉴스에서의 해당 단어의 빈도수를 의미\n",
    "- 이는 gensim의 corpora.Dictionary()를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\mytest\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(52, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 2), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 2), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 2), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 2), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1)]\n"
     ]
    }
   ],
   "source": [
    "# pip install gensim\n",
    "\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(tokenized_doc)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
    "print(corpus[1]) # 수행된 결과에서 두번째 뉴스 출력, 첫번째 문서의 인덱스는 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두 번째 뉴스의 출력 결과, (66, 2)는 정수 인코딩이 66으로 할당된 단어가 두번째 뉴스에서 두번 등장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dostoevsky\n"
     ]
    }
   ],
   "source": [
    "# 66이라는 단어가 어떤 단어인지 확인\n",
    "print(dictionary[66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70484"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 총 학습된 단어의 개수 확인\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) LDA 모델 훈련시키기\n",
    "- 토픽 개수를 20으로 하여 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.011*\"chicago\" + 0.009*\"period\" + 0.008*\"flames\" + 0.008*\"calgary\"')\n",
      "(1, '0.014*\"government\" + 0.011*\"president\" + 0.009*\"states\" + 0.008*\"american\"')\n",
      "(2, '0.011*\"forsale\" + 0.008*\"maynard\" + 0.008*\"rows\" + 0.008*\"deck\"')\n",
      "(3, '0.012*\"would\" + 0.012*\"people\" + 0.008*\"think\" + 0.006*\"many\"')\n",
      "(4, '0.027*\"drive\" + 0.021*\"card\" + 0.018*\"disk\" + 0.018*\"scsi\"')\n",
      "(5, '0.011*\"cover\" + 0.009*\"rochester\" + 0.009*\"copies\" + 0.008*\"john\"')\n",
      "(6, '0.030*\"wire\" + 0.023*\"ground\" + 0.017*\"wiring\" + 0.016*\"neutral\"')\n",
      "(7, '0.012*\"armenian\" + 0.011*\"israel\" + 0.011*\"said\" + 0.010*\"turkish\"')\n",
      "(8, '0.008*\"power\" + 0.007*\"bike\" + 0.007*\"sale\" + 0.006*\"used\"')\n",
      "(9, '0.032*\"jesus\" + 0.015*\"christian\" + 0.013*\"bible\" + 0.013*\"christ\"')\n",
      "(10, '0.018*\"insurance\" + 0.015*\"gordon\" + 0.013*\"pitt\" + 0.012*\"pain\"')\n",
      "(11, '0.021*\"would\" + 0.018*\"know\" + 0.017*\"like\" + 0.010*\"anyone\"')\n",
      "(12, '0.020*\"game\" + 0.018*\"team\" + 0.016*\"year\" + 0.014*\"games\"')\n",
      "(13, '0.039*\"university\" + 0.016*\"dept\" + 0.015*\"internet\" + 0.012*\"henrik\"')\n",
      "(14, '0.035*\"space\" + 0.017*\"nasa\" + 0.008*\"earth\" + 0.007*\"research\"')\n",
      "(15, '0.013*\"encryption\" + 0.012*\"chip\" + 0.011*\"keys\" + 0.010*\"security\"')\n",
      "(16, '0.009*\"period\" + 0.008*\"jets\" + 0.007*\"picture\" + 0.007*\"scorer\"')\n",
      "(17, '0.009*\"guns\" + 0.006*\"case\" + 0.005*\"weapons\" + 0.005*\"contest\"')\n",
      "(18, '0.033*\"church\" + 0.013*\"catholic\" + 0.009*\"pope\" + 0.009*\"georgia\"')\n",
      "(19, '0.012*\"file\" + 0.009*\"program\" + 0.008*\"available\" + 0.006*\"information\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 20 # 토픽 개수 k = 20\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 단어 앞에 붙은 수치 : 단어의 해당 토픽에 대한 기여도\n",
    "- passes : 알고리즘의 동작 횟수 (알고리즘이 결정하는 토픽의 값이 적절히 수렴할 수 있도록 적당한 횟수 지정)\n",
    "- num_words=4 : 총 4개의 단어만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA 시각화 하기\n",
    "- 시각화 : pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
