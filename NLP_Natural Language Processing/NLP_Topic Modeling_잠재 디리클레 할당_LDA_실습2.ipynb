{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 잠재 디리클레 할당(LDA) 실습\n",
    "- LDA를 사이킷런을 사용하여 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1) 뉴스 기사 제목 데이터에 대한 이해</b>\n",
    "- 15년간 발행된 뉴스 기사 제목을 다운로드\n",
    "- 링크 : https://www.kaggle.com/therohk/million-headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('C:/Users/fxk/PycharmProjects/tenjumh/Study/NLP_Natural Language Processing/data/abcnews-date-text.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1103663\n",
      "   publish_date                                      headline_text\n",
      "0      20030219  aba decides against community broadcasting lic...\n",
      "1      20030219     act fire witnesses must be aware of defamation\n",
      "2      20030219     a g calls for infrastructure protection summit\n",
      "3      20030219           air nz staff in aust strike for pay rise\n",
      "4      20030219      air nz strike to affect australian travellers\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(data.head(5))\n",
    "# publish_data(날짜(와 headline_text(기사 제목) 두 개의 열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 기사 제목만 필요함으로 별도 저장\n",
    "text = data[['headline_text']]\n",
    "text1 = data['headline_text']   # 이렇게하면 컬럼 정보가 사라짐 따라서 위에 같이..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    aba decides against community broadcasting lic...\n",
       "1       act fire witnesses must be aware of defamation\n",
       "2       a g calls for infrastructure protection summit\n",
       "3             air nz staff in aust strike for pay rise\n",
       "4        air nz strike to affect australian travellers\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1103663,)\n",
      "(1103663, 1)\n"
     ]
    }
   ],
   "source": [
    "print(text1.shape)\n",
    "print(text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2) 텍스트 전처리</b>\n",
    "- 불용어 제거, 표제어 추출, 짧은 단어 제거 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\mytest\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 단어 토큰화 수행\n",
    "text['headline_text'] = text.apply(lambda row: nltk.word_tokenize(row['headline_text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       headline_text\n",
      "0  [aba, decides, against, community, broadcastin...\n",
      "1  [act, fire, witnesses, must, be, aware, of, de...\n",
      "2  [a, g, calls, for, infrastructure, protection,...\n",
      "3  [air, nz, staff, in, aust, strike, for, pay, r...\n",
      "4  [air, nz, strike, to, affect, australian, trav...\n"
     ]
    }
   ],
   "source": [
    "print(text.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       headline_text\n",
      "0   [aba, decides, community, broadcasting, licence]\n",
      "1    [act, fire, witnesses, must, aware, defamation]\n",
      "2     [g, calls, infrastructure, protection, summit]\n",
      "3          [air, nz, staff, aust, strike, pay, rise]\n",
      "4  [air, nz, strike, affect, australian, travellers]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\mytest\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop)])\n",
    "print(text.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       headline_text\n",
      "0       [aba, decide, community, broadcast, licence]\n",
      "1      [act, fire, witness, must, aware, defamation]\n",
      "2      [g, call, infrastructure, protection, summit]\n",
      "3          [air, nz, staff, aust, strike, pay, rise]\n",
      "4  [air, nz, strike, affect, australian, travellers]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\mytest\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# 표제어 추출 : 3인칭 단수를 1인칭 변경, 과거형 동사를 현재형으로 변경\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
    "print(text.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [decide, community, broadcast, licence]\n",
      "1      [fire, witness, must, aware, defamation]\n",
      "2    [call, infrastructure, protection, summit]\n",
      "3                   [staff, aust, strike, rise]\n",
      "4      [strike, affect, australian, travellers]\n",
      "Name: headline_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 짧은 단어 삭제\n",
    "\n",
    "tokenized_doc = text['headline_text'].apply(lambda x: [word for word in x if len(word) > 3])\n",
    "print(tokenized_doc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3) TF-IDF</b>\n",
    "- TF-IDF에 TfidfVectorize는 기본적으로 토큰화되어 있지 않은 텍스트 데이터를 입력으로 사용\n",
    "- 따라서 전처리를 위해 했던 토큰화 작업을 역으로 취소하는 \"역토큰화\"작업 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\mytest\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# 역토큰화 (토큰화 작업을 되돌림)\n",
    "detokenized_doc = []\n",
    "for i  in range(len(text)):   # 왜 불용어, 인칭 통일 작업까지 한 녀석으로 하지?\n",
    "    t = ' '.join(tokenized_doc[i])   # str.join()은 \"Study/Python Function Collection\" 참조\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "text['headline_text'] = detokenized_doc  # 다시 text['headline_text']에 재저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       decide community broadcast licence\n",
       "1       fire witness must aware defamation\n",
       "2    call infrastructure protection summit\n",
       "3                   staff aust strike rise\n",
       "4      strike affect australian travellers\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text['headline_text'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 역토큰화가 수행됨\n",
    "- 사이킷런의 TfidfVectorizer를 이용하여 TF-IDF행렬 생성\n",
    "- 시간이 오래걸리니 1000개의 단어로만 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1103663, 1)\n",
      "(1103663,)\n",
      "(1103663, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)  #상위 1000개 단어 보존\n",
    "X = vectorizer.fit_transform(text['headline_text'])\n",
    "print(text.shape)\n",
    "print(tokenized_doc.shape)\n",
    "print(X.shape)     # 1103663개 뉴스에, 뉴스 당 1000개 단어만"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3) 토픽 모델링</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_model = LatentDirichletAllocation(n_components=10, learning_method='online', random_state=777, max_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000703e-01 1.00000829e-01 1.00003578e-01 ... 1.00004871e-01\n",
      "  1.00003129e-01 1.00002930e-01]\n",
      " [1.00001421e-01 8.66862951e+02 1.00008903e-01 ... 1.00004224e-01\n",
      "  1.00005598e-01 7.01841034e+02]\n",
      " [1.00000648e-01 1.00000545e-01 1.00002661e-01 ... 1.00005158e-01\n",
      "  1.00008596e-01 1.00001987e-01]\n",
      " ...\n",
      " [1.00001636e-01 1.00000889e-01 2.68570402e+03 ... 1.00003039e-01\n",
      "  1.00010511e-01 1.00004475e-01]\n",
      " [1.00001352e-01 1.00000852e-01 1.00003353e-01 ... 1.00003378e-01\n",
      "  1.00005211e-01 1.00003635e-01]\n",
      " [1.00002244e-01 1.00000967e-01 1.00003675e-01 ... 1.00002444e-01\n",
      "  1.00003580e-01 1.00004738e-01]]\n",
      "(10, 1000)\n"
     ]
    }
   ],
   "source": [
    "lda_top = lda_model.fit_transform(X)\n",
    "print(lda_model.components_)\n",
    "print(lda_model.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('government', 8658.95), ('queensland', 8134.58), ('perth', 6332.45), ('year', 5981.93), ('change', 5833.07)]\n",
      "Topic 2: [('world', 7026.33), ('house', 6217.97), ('donald', 5757.52), ('open', 5620.39), ('years', 5563.76)]\n",
      "Topic 3: [('police', 12140.34), ('kill', 6091.65), ('interview', 5921.12), ('live', 5657.67), ('rise', 4162.16)]\n",
      "Topic 4: [('court', 6173.46), ('crash', 5497.33), ('state', 4857.9), ('tasmania', 4443.89), ('accuse', 4300.92)]\n",
      "Topic 5: [('australia', 13994.07), ('south', 6253.18), ('woman', 5614.31), ('coast', 5465.23), ('warn', 5155.11)]\n",
      "Topic 6: [('charge', 8440.62), ('election', 7650.47), ('adelaide', 6839.75), ('murder', 6418.61), ('make', 6198.2)]\n",
      "Topic 7: [('help', 5372.6), ('miss', 4601.06), ('people', 4561.71), ('2016', 4212.58), ('family', 4149.3)]\n",
      "Topic 8: [('sydney', 8597.95), ('melbourne', 7603.52), ('canberra', 6285.91), ('plan', 5606.37), ('power', 4198.99)]\n",
      "Topic 9: [('attack', 6818.74), ('market', 5094.55), ('council', 3854.2), ('share', 3811.79), ('national', 3793.87)]\n",
      "Topic 10: [('trump', 13043.55), ('australian', 11389.92), ('north', 6241.08), ('school', 5726.25), ('report', 5560.48)]\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\"%(idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "\n",
    "get_topics(lda_model.components_,terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
