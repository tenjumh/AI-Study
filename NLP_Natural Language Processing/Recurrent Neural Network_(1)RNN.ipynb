{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "- RNN을 뉴런 단위로 시각화\n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/RNN/rnn1.PNG?raw=True)\n",
    "- 다양한 RNN 쓰임\n",
    "    1. 스팸 메일 분류\n",
    "    \n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/RNN/rnn3.PNG?raw=True)\n",
    "\n",
    "    2. 개체명 인식\n",
    "    \n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/RNN/rnn4.PNG?raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN 수식 정의\n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/RNN/rnn5.PNG?raw=True)\n",
    "    1. 현 시점 t에서의 은닉 상태값을 h(t)라고 정의\n",
    "    2. h(t)를 계산하기 위해서는 두 개의 가중치가 필요\n",
    "    3. w(x)와 t-1시점의 은닉 상태값인 w(h)\n",
    "    4. 식으로 표현하면\n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/RNN/rnn6.PNG?raw=True)\n",
    "    5. 자연어 처리에서 RNN의 입력 x(t)는 대부분 단어 벡터로 간주, 단어벡터의 차원을 d라고 하고, 은닉 상태의 크기를 D(h)라고 할 때,\n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/RNN/rnn7.PNG?raw=True)\n",
    "    6. 배치 크기가 1이고 d와 D(h) 두 값을 모두 4로 가정하면 그림은,\n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/RNN/rnn8.PNG?raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 케라스로 RNN 구현하기\n",
    "- RNN층을 추가하는 코드\n",
    "```\n",
    "model.add(SimpleRNN(hidden_size)\n",
    "```\n",
    "- 추가 인자를 사용할 때\n",
    "```\n",
    "model.add(SimpleRNN(hidden_size, input_shape=(timesteps, input_dim)))\n",
    "```\n",
    "- 다른 표기\n",
    "```\n",
    "model.add(SimpleRNN(hidden_size, input_length=M, input_dim=N))  # M과 N은 정수\n",
    "```\n",
    "- hidden_size = 은닉상태의 크기 (메모리셀 = 다음 시점 메모리셀 = 출력층 보내는 값(output_dim) 크기 동일), 보통(128, 256, 512, 1024)\n",
    "- timesteps = 입력 시퀀스 길이(input_length)\n",
    "- input_dim = 입력의 크기\n",
    "\n",
    "- RNN층은 input the 3D Tensor(batch_size, timesteps, input_dim)\n",
    "- RNN층은 두 가지 종류 출력, <br>\n",
    "    (1) 최종 시점의 은닉 상태만 리턴 - 2D 텐서(batch_size, output_dim) 리턴, <br>\n",
    "    (2) 각 시점(time step)의 은닉 상태값들을 모아 전체 시퀀스를 리턴 - 3D 텐서(batch_size, timesteps, output_dim) 리턴<br>\n",
    "- return_sequences 매개 변수에 True 설정하여 설정 가능\n",
    "\n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/RNN/return_sequences.PNG?raw=True)\n",
    "- 위 그림은 time step=3일 때, return_sequences=Ture와 False로 설정했을 때 차이\n",
    "- True : 모든 time step에서 은닉 상태값을 출력 -- * many-to-many\n",
    "- False or X : 마지막 time step의 은닉 상태값을 출력 -- * many-to-one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, input_shape=(2,10)))\n",
    "# model.add(SimpleRNN(3, input_length=2, input_dim=10)) 동일\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- output shape이 (None, 3) -> batch_size가 정해지지 않아서 None이며 3은 hidden_size이다.\n",
    "- 그리고 return_sequences를 기재하지 않아 False로 진행되어 <b><h2> \"2D\" </h2></b>output이 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (8, 3)                    42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, batch_input_shape=(8, 2, 10)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch_size를 정의하면 (8, 3)임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_7 (SimpleRNN)     (8, 2, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, batch_input_shape=(8, 2, 10), return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- return_sequences을 True로 하여 (batch_size, timesteps, output_dim) 크기의 <b><h2> \"3D\" </h2></b>텐서를 리턴한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이썬으로 RNN 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메모리 셀에서 은닉 상태를 계산하는 식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/RNN/rnn9.PNG?raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "timesteps = 10   # 이거슨 NLP에서 문장의 길이가 된다. 예 \"나는 학교에 간다\" 는 3이죠.\n",
    "input_dim = 4   # 단어 벡터의 차원이다. 나는 = [1,0,0,0]   4차원\n",
    "hidden_size = 8   # 메모리 셀의 용량\n",
    "\n",
    "inputs = np.random.random((timesteps, input_dim))   # 입력에 해당하는 2D 텐서\n",
    "\n",
    "hidden_state_t = np.zeros((hidden_size,))   # 초기 은닉 상태는 0벡터로 초기화\n",
    "# hidden_stata_t의 상태 크기는 hidden_size로 은닉 상태를 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(hidden_state_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치와 편향 정의\n",
    "\n",
    "Wx = np.random.random((hidden_size, input_dim))    # (8, 4) : 4차원 벡터값이 8개의 은닉층으로 들어가서\n",
    "Wh = np.random.random((hidden_size, hidden_size))     # (8, 8) : t-1 은닉층의 8개가 t 은닉층의 8개로 들어가서\n",
    "b = np.random.random((hidden_size,))    # (8,)크기의 1D 텐서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n"
     ]
    }
   ],
   "source": [
    "print(Wx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8)\n"
     ]
    }
   ],
   "source": [
    "print(Wh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "(2, 8)\n",
      "(3, 8)\n",
      "(4, 8)\n",
      "(5, 8)\n",
      "(6, 8)\n",
      "(7, 8)\n",
      "(8, 8)\n",
      "(9, 8)\n",
      "(10, 8)\n",
      "[[0.99998468 0.99998412 0.99997676 0.99998541 0.99997305 0.99995806\n",
      "  0.99983635 0.99999874]\n",
      " [0.99998461 0.99997635 0.99996817 0.99998651 0.99996796 0.99996505\n",
      "  0.99987546 0.99999881]\n",
      " [0.99999255 0.99998893 0.99999006 0.99999497 0.99998866 0.99997895\n",
      "  0.99994091 0.99999953]\n",
      " [0.99995534 0.99996155 0.99997411 0.99998753 0.99996247 0.99996209\n",
      "  0.99980169 0.99999882]\n",
      " [0.9999755  0.99996368 0.99997769 0.99999045 0.99997417 0.99998026\n",
      "  0.99991953 0.99999916]\n",
      " [0.99999236 0.9999869  0.99999141 0.9999936  0.99999101 0.99999017\n",
      "  0.99997015 0.99999951]\n",
      " [0.99996431 0.99997469 0.99996991 0.99996857 0.99996113 0.99996215\n",
      "  0.99977801 0.99999772]\n",
      " [0.99997979 0.99998353 0.999986   0.99998814 0.99998068 0.99997195\n",
      "  0.99987384 0.99999901]\n",
      " [0.99997934 0.99998384 0.99997714 0.99996414 0.99997372 0.99997385\n",
      "  0.99986136 0.99999776]\n",
      " [0.99998529 0.99998412 0.99999196 0.99999081 0.99998978 0.99999008\n",
      "  0.99995854 0.99999935]]\n"
     ]
    }
   ],
   "source": [
    "total_hidden_states = []\n",
    "\n",
    "# 메모리 셀 동작\n",
    "for input_t in inputs:  # 각 시점에 따라서 입력값이 입력\n",
    "    output_t = np.tanh(np.dot(Wx, input_t) + np.dot(Wh, hidden_state_t) + b)\n",
    "    # W(x)*X(t) + W(h)*H(t-1) + b\n",
    "    total_hidden_states.append(list(output_t))   # 각 시점의 은닉 상태의 값을 계속 추적\n",
    "    print(np.shape(total_hidden_states))  # 각 시점 t별 메모리 셀의 출력의 크기\n",
    "    hidden_state_t = output_t\n",
    "    \n",
    "total_hidden_states = np.stack(total_hidden_states, axis = 0)\n",
    "# 출력 시 값을 깔끔하게 해준다.\n",
    "\n",
    "print(total_hidden_states)  # (timesteps, output_dim)의 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 양방향 순환 신경망 (Bidirectional Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 양방향 순환 신경망 : 시점 t에서의 출력값을 예측할 때 이전 시점의 데이터뿐만 아니라, 이후 데이터로도 예측할 수 있다는 아이디어\n",
    "- 영어 빈칸 채우기 문제에 비유\n",
    "    Exercise is very effective at [        ] belly fat.<br>\n",
    "    \n",
    "    1) reducing<br>\n",
    "    2) increasing<br>\n",
    "    3) multiplying<br>\n",
    "- 정답을 찾기 위해서는 이전 단어들만으로는 부족, 이후 단어인 belly fat을 봐야함.\n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/RNN/rnn11.PNG?raw=True)\n",
    "- 양방향 RNN은 하나의 출력값을 예측하기 위해 기본적으로 두 개의 메모리 셀을 사용\n",
    "- 첫번째 메모리 셀은 앞에서 배운 것처럼 \"앞 시점의 은닉 상태(Forward States)\"를 전달받아 현재의 은닉 상태를 계산\n",
    "- 두번째 메모리 셀은 앞 시점의 은닉 상태가 아니라 뒤 시점의 은닉 상태(Backward States)를 전달 받아 현재의 은닉 상태를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True), input_shape=(timesteps, input_dim)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 양방향 RNN의 은닉층이 두개인 경우 그림과 코드\n",
    "![doc1](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/RNN/rnn12.PNG?raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True), input_shape=(timesteps, input_dim)))\n",
    "model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras RNN에 대한 매우 자세한 설명\n",
    "https://stackoverflow.com/questions/38714959/understanding-keras-lstms\n",
    "https://gluon.mxnet.io/chapter05_recurrent-neural-networks/simple-rnn.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
