{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 표현 방법\n",
    "- Local Representation(=Discrete Representation)<br>\n",
    " 1) One-hot Vector<br>\n",
    " 2) N-gram<br>\n",
    " 3) Count Based - Bag of Words(DTM)<br>\n",
    "- Continuous Representaion(=Distributed Representation)<br>\n",
    " 1) Prediction Based - Word2Vec<br>\n",
    " 2) Count Based - LSA(Full Document), Glove(Windows)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bag of Words(BoW)\n",
    "- 단어의 순서를 고려 X, 출현 빈도(frequency)에만 집중 : 텍스트 데이터의 수치화 표현\n",
    "- BoW 만드는 과정<br>\n",
    " 1) 각 단어에 고유한 정수 인덱스를 부여<br>\n",
    " 2) 인덱스의 위치에 단어 트큰의 등장 횟수를 기록한 벡터<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n",
      "[1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "okt = Okt()\n",
    "\n",
    "token = re.sub(\"(\\.)\",\"\",\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")\n",
    "#정규 표현식을 통해 온점을 제거하는 정제 작업입니다.\n",
    "#print(token)\n",
    "token = okt.morphs(token)\n",
    "#OKT 형태소 분석기를 통해 토큰화 작업을 수행한 뒤에, token에다 넣기\n",
    "\n",
    "word2index = {}\n",
    "bow = []\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index)\n",
    "        #token을 읽으면서, word2index에 없는(not in) 단어는 새로 추가, 있는 단어는 pass\n",
    "        bow.insert(len(word2index)-1, 1)\n",
    "        #BoW 전체에 전부 기본값을 1을 넣어줍니다. 단어 개수는 최소 1개 이상이기에\n",
    "    else:\n",
    "        index = word2index.get(voca)\n",
    "        #재등장하는 단어의 인덱스를 받아옵니다.\n",
    "        bow[index] = bow[index] + 1\n",
    "        #재등장한 단어는 해당하는 인덱스 위치에 1더해줌\n",
    "print(word2index)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "okt = Okt()\n",
    "\n",
    "token3 = re.sub(\"(\\.)\",\"\",\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다. 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\")\n",
    "token2 = re.sub(\"(\\.)\",\"\",\"소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\")\n",
    "token1 = re.sub(\"(\\.)\",\"\",\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")\n",
    "#정규 표현식을 통해 온점을 제거하는 정제 작업입니다.\n",
    "#print(token)\n",
    "token3 = okt.morphs(token3)\n",
    "token2 = okt.morphs(token2)\n",
    "token1 = okt.morphs(token1)\n",
    "#OKT 형태소 분석기를 통해 토큰화 작업을 수행한 뒤에, token에다 넣기\n",
    "\n",
    "word2index = {}\n",
    "bow = []\n",
    "for voca in token3:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index)\n",
    "        #token을 읽으면서, word2index에 없는(not in) 단어는 새로 추가, 있는 단어는 pass\n",
    "        bow.insert(len(word2index)-1, 0)\n",
    "        #BoW 전체에 전부 기본값을 1을 넣어줍니다. 단어 개수는 최소 1개 이상이기에\n",
    "\n",
    "bow2 = bow[:]   #리스트 복사할 때 이렇게 해라\n",
    "bow3 = bow[:]\n",
    "\n",
    "for voca in token3:\n",
    "    if voca in word2index.keys():\n",
    "        index = word2index.get(voca)\n",
    "        #재등장하는 단어의 인덱스를 받아옵니다.\n",
    "        bow[index] = bow[index] + 1\n",
    "        #재등장한 단어는 해당하는 인덱스 위치에 1더해줌\n",
    "\n",
    "for voca in token1:\n",
    "    if voca in word2index.keys():\n",
    "        index = word2index.get(voca)\n",
    "        #재등장하는 단어의 인덱스를 받아옵니다.\n",
    "        bow2[index] = bow2[index] + 1\n",
    "        #재등장한 단어는 해당하는 인덱스 위치에 1더해줌\n",
    "\n",
    "for voca in token2:\n",
    "    if voca in word2index.keys():\n",
    "        index = word2index.get(voca)\n",
    "        #재등장하는 단어의 인덱스를 받아옵니다.\n",
    "        bow3[index] = bow3[index] + 1\n",
    "        #재등장한 단어는 해당하는 인덱스 위치에 1더해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17}\n"
     ]
    }
   ],
   "source": [
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token3: [1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]\n",
      "token2: [1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "token1: [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"token3:\", bow)\n",
    "print(\"token2:\", bow2)\n",
    "print(\"token1:\", bow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW는 각 단어가 등장한 횟수를 수치화하는 텍스트 표현 방법이기 때문에, 주로 어떤 단어가 얼마나 등장했는지를 기준으로 문서가 어떤 성격의 문서인지를 판단하는 작업에 쓰인다. 즉, 분류 문제나 여러 문서 간의 유사도를 구하는 문제에 주로 쓰인다. 가령, '달리기', '체력', '근력'과 같은 단어가 자주 등장하면 해당 문서를 체육 관련 문서로 분류할 수 있을 것이며, '미분', '방정식', '부등식'과 같은 단어가 자주 등장한다면 수학 관련 문서로 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CountVectorizer 클래스로 BoW 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 2 1]]\n",
      "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
    "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer는 단지 띄어쓰기만을 기준으로 단어를 자르는 낮은 수준의 토큰화를 진행<br>\n",
    "따라서 한글을 할 경우 '물가상승률과'와 '물가상승률은'는 다른 두 단어로 인식하여 한글에는 적합하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 불용어를 제거한 BoW 만들기\n",
    "- 불용어는 자연어 처리에서 별로 의미를 갖지 않는 단어들\n",
    "- 영어의 BoW를 만들기 위해 사용하는 CountVectorizer는 불용어를 지정하면, 불용어는 제외하고 BoW를 만들 수 있도록 불용어 제거 기능을 지원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 사용자가 직접 정의한 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
    "print(vect.fit_transform(text).toarray()) \n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) CounterVectorizer에서 제공하는 자체 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]]\n",
      "{'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) NLTK에서 지원하는 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "vect = CountVectorizer(stop_words =sw)\n",
    "print(vect.fit_transform(text).toarray()) \n",
    "print(vect.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
