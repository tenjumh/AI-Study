{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>5. 실습을 통한 이해</b><br>\n",
    "- LDA는 gensim 사용\n",
    "\n",
    "1) 정수 인코딩과 단어 집합 만들기\n",
    "- LSA챕터에서 사용한 Twenty Newsgroups 데이터 및 전처리 과정을 활용\n",
    "- tokenized_doc으로 저장한 상태부터 시작함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers','footeers','quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)\n",
    "print(dataset.target_names)\n",
    "\n",
    "# 텍스트 전처리\n",
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\",\" \")\n",
    "# 길이가 3이하인 단어 제거\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
    "\n",
    "# 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "# NLTK로부터 불용어 받아옴\n",
    "stop_words = stopwords.words('english')\n",
    "# 토큰화\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "# 불용어 제거\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [well, sure, story, seem, biased, disagree, st...\n",
       "1    [yeah, expect, people, read, actually, accept,...\n",
       "2    [although, realize, principle, strongest, poin...\n",
       "3    [notwithstanding, legitimate, fuss, proposal, ...\n",
       "4    [well, change, scoring, playoff, pool, unfortu...\n",
       "Name: clean_doc, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련용 뉴스 5개만 출력\n",
    "tokenized_doc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 단어에 정수 인코딩을 하는 동시에 각 뉴스에서의 단어 빈도수를 기록\n",
    "- 각 단어를 (word_id, word_frequency)의 형태로 바꿈\n",
    "- word_id는 단어가 정수 인코딩된 값이고 word_frequency는 해당 뉴스에서의 해당 단어의 빈도수를 의미\n",
    "- 이는 gensim의 corpora.Dictionary()를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(52, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 2), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 2), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 2), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 2), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1)]\n"
     ]
    }
   ],
   "source": [
    "# pip install gensim\n",
    "\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(tokenized_doc)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
    "print(corpus[1]) # 수행된 결과에서 두번째 뉴스 출력, 첫번째 문서의 인덱스는 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두 번째 뉴스의 출력 결과, (66, 2)는 정수 인코딩이 66으로 할당된 단어가 두번째 뉴스에서 두번 등장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dostoevsky\n"
     ]
    }
   ],
   "source": [
    "# 66이라는 단어가 어떤 단어인지 확인\n",
    "print(dictionary[66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70484"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 총 학습된 단어의 개수 확인\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) LDA 모델 훈련시키기\n",
    "- 토픽 개수를 20으로 하여 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.020*\"game\" + 0.018*\"team\" + 0.014*\"games\" + 0.014*\"year\"')\n",
      "(1, '0.025*\"space\" + 0.011*\"nasa\" + 0.007*\"data\" + 0.005*\"program\"')\n",
      "(2, '0.008*\"colormap\" + 0.008*\"mask\" + 0.007*\"germany\" + 0.007*\"sweden\"')\n",
      "(3, '0.015*\"jesus\" + 0.009*\"christian\" + 0.008*\"bible\" + 0.008*\"church\"')\n",
      "(4, '0.028*\"university\" + 0.012*\"research\" + 0.012*\"health\" + 0.010*\"medical\"')\n",
      "(5, '0.014*\"government\" + 0.013*\"president\" + 0.010*\"state\" + 0.010*\"states\"')\n",
      "(6, '0.009*\"would\" + 0.007*\"much\" + 0.007*\"like\" + 0.007*\"good\"')\n",
      "(7, '0.009*\"slave\" + 0.008*\"doug\" + 0.008*\"master\" + 0.007*\"dyer\"')\n",
      "(8, '0.013*\"fire\" + 0.010*\"never\" + 0.010*\"koresh\" + 0.009*\"tobacco\"')\n",
      "(9, '0.029*\"period\" + 0.017*\"power\" + 0.013*\"play\" + 0.013*\"scorer\"')\n",
      "(10, '0.013*\"file\" + 0.008*\"program\" + 0.008*\"windows\" + 0.007*\"available\"')\n",
      "(11, '0.010*\"food\" + 0.006*\"cubs\" + 0.005*\"kevin\" + 0.004*\"scores\"')\n",
      "(12, '0.026*\"georgia\" + 0.019*\"water\" + 0.013*\"rutgers\" + 0.011*\"gatech\"')\n",
      "(13, '0.011*\"bike\" + 0.008*\"engine\" + 0.007*\"good\" + 0.006*\"ride\"')\n",
      "(14, '0.013*\"people\" + 0.008*\"would\" + 0.006*\"many\" + 0.006*\"israel\"')\n",
      "(15, '0.023*\"filename\" + 0.014*\"kent\" + 0.009*\"newton\" + 0.008*\"nick\"')\n",
      "(16, '0.019*\"armenian\" + 0.016*\"turkish\" + 0.016*\"armenians\" + 0.011*\"said\"')\n",
      "(17, '0.023*\"drive\" + 0.016*\"card\" + 0.015*\"disk\" + 0.015*\"scsi\"')\n",
      "(18, '0.020*\"would\" + 0.015*\"know\" + 0.014*\"like\" + 0.013*\"think\"')\n",
      "(19, '0.011*\"encryption\" + 0.010*\"public\" + 0.010*\"chip\" + 0.009*\"security\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 20 # 토픽 개수 k = 20\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 단어 앞에 붙은 수치 : 단어의 해당 토픽에 대한 기여도\n",
    "- passes : 알고리즘의 동작 횟수 (알고리즘이 결정하는 토픽의 값이 적절히 수렴할 수 있도록 적당한 횟수 지정)\n",
    "- num_words=4 : 총 4개의 단어만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA 시각화 하기\n",
    "- 시각화 : pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 문서의 topic 비율은 [(2, 0.017979898), (4, 0.09672378), (14, 0.48671848), (15, 0.09900035), (16, 0.11860768), (18, 0.16947204)]\n",
      "1 번째 문서의 topic 비율은 [(3, 0.08425105), (11, 0.19295196), (12, 0.020964203), (14, 0.29460165), (18, 0.39221802)]\n",
      "2 번째 문서의 topic 비율은 [(4, 0.051485468), (5, 0.02315645), (6, 0.017746411), (14, 0.4417252), (17, 0.025380848), (18, 0.36311036), (19, 0.06711812)]\n",
      "3 번째 문서의 topic 비율은 [(1, 0.041216128), (5, 0.06328405), (6, 0.26983684), (7, 0.030355416), (11, 0.015257737), (14, 0.05285601), (17, 0.029109165), (18, 0.1637964), (19, 0.3261525)]\n",
      "4 번째 문서의 topic 비율은 [(0, 0.4325652), (6, 0.26096943), (18, 0.2781178)]\n"
     ]
    }
   ],
   "source": [
    "for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(i, '번째 문서의 topic 비율은', topic_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (숫자, 확률)은 각각 토픽 번호와 해당 토픽이 해당 문서에서 차지하는 분포\n",
    "- 예를 들어, 0번째 문서의 토픽에서 (2, 0.017979898)은 2번 토픽이 1.7%의 분포를 (14, 0.48671848)는 14번 토픽이 48.7%의 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임 형식으로 출력\n",
    "def make_topictable_per_doc(ldamodel, corpus, texts):\n",
    "    topic_table = pd.DataFrame()\n",
    "    \n",
    "    #몇 번째 문서인지를 의미하는 문서 번호와 해당 문서의 토픽 비중을 한 줄씩 꺼내옴\n",
    "    for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "        doc = topic_list[0] if ldamodel.per_word_topics else topic_list\n",
    "        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n",
    "        # 각 문서에 대해서 비중이 높은 토픽순으로 정렬\n",
    "        # 예) 정렬 전 0번 문서 : [(2번 토픽, 1.7%), (4번 토픽, 9.6%), (14번 토픽, 48.7%) 였다면]\n",
    "        #     정렬 후 0번 문서 : [(14번 토픽, 48.7%), (4번 토픽, 9.6%), (2번 토픽, 1.7%) 순으로 정렬]\n",
    "        \n",
    "        # 모든 문서에 대해서 각 아래를 수행\n",
    "        for j, (topic_num, prop_topic) in enumerate(doc):  # 몇 번 토픽인지와 비중을 나눠서 저장\n",
    "            if j == 0:  # 정렬이 완료된 상태이므로 가장 앞에 있는 것이 비중이 가장 높은 토픽\n",
    "                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic, 4), topic_list]), ignore_index=True)\n",
    "                # 가장 비중이 높은 토픽과 비중 그리고 전체 토픽의 비중을 저장\n",
    "            else:\n",
    "                break\n",
    "    return(topic_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0       1                                                  2\n",
      "0  14.0  0.4867  [(2, 0.01797956), (4, 0.09671218), (14, 0.4867...\n",
      "1  18.0  0.3923  [(3, 0.08424776), (11, 0.1929529), (12, 0.0209...\n",
      "2  14.0  0.4417  [(4, 0.051490474), (5, 0.023157403), (6, 0.017...\n",
      "3  19.0  0.3262  [(1, 0.04121628), (5, 0.0632835), (6, 0.269845...\n",
      "4   0.0  0.4326  [(0, 0.43258402), (6, 0.26110706), (18, 0.2779...\n",
      "5   3.0  0.5310  [(2, 0.017816834), (3, 0.53102016), (6, 0.0819...\n",
      "6  10.0  0.3815  [(6, 0.26749086), (10, 0.38151368), (15, 0.036...\n",
      "7  18.0  0.4917  [(5, 0.055641364), (8, 0.034521755), (11, 0.01...\n",
      "8  18.0  0.3335  [(4, 0.051082034), (7, 0.17797029), (10, 0.182...\n",
      "9  18.0  0.3927  [(2, 0.015176013), (4, 0.14343473), (6, 0.2628...\n",
      "================================================================\n",
      "   index     0       1                                                  2\n",
      "0      0  14.0  0.4867  [(2, 0.01797956), (4, 0.09671218), (14, 0.4867...\n",
      "1      1  18.0  0.3923  [(3, 0.08424776), (11, 0.1929529), (12, 0.0209...\n",
      "2      2  14.0  0.4417  [(4, 0.051490474), (5, 0.023157403), (6, 0.017...\n",
      "3      3  19.0  0.3262  [(1, 0.04121628), (5, 0.0632835), (6, 0.269845...\n",
      "4      4   0.0  0.4326  [(0, 0.43258402), (6, 0.26110706), (18, 0.2779...\n",
      "5      5   3.0  0.5310  [(2, 0.017816834), (3, 0.53102016), (6, 0.0819...\n",
      "6      6  10.0  0.3815  [(6, 0.26749086), (10, 0.38151368), (15, 0.036...\n",
      "7      7  18.0  0.4917  [(5, 0.055641364), (8, 0.034521755), (11, 0.01...\n",
      "8      8  18.0  0.3335  [(4, 0.051082034), (7, 0.17797029), (10, 0.182...\n",
      "9      9  18.0  0.3927  [(2, 0.015176013), (4, 0.14343473), (6, 0.2628...\n"
     ]
    }
   ],
   "source": [
    "topictable = make_topictable_per_doc(ldamodel, corpus, tokenized_doc)\n",
    "print(topictable[:10])\n",
    "topictable = topictable.reset_index()  # 문서 번호를 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만듬\n",
    "print(\"================================================================\")\n",
    "print(topictable[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문서 번호</th>\n",
       "      <th>가장 비중이 높은 토픽</th>\n",
       "      <th>가장 높은 토픽의 비중</th>\n",
       "      <th>각 토픽의 비중</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.4867</td>\n",
       "      <td>[(2, 0.01797956), (4, 0.09671218), (14, 0.4867...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.3923</td>\n",
       "      <td>[(3, 0.08424776), (11, 0.1929529), (12, 0.0209...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.4417</td>\n",
       "      <td>[(4, 0.051490474), (5, 0.023157403), (6, 0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.3262</td>\n",
       "      <td>[(1, 0.04121628), (5, 0.0632835), (6, 0.269845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4326</td>\n",
       "      <td>[(0, 0.43258402), (6, 0.26110706), (18, 0.2779...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5310</td>\n",
       "      <td>[(2, 0.017816834), (3, 0.53102016), (6, 0.0819...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.3815</td>\n",
       "      <td>[(6, 0.26749086), (10, 0.38151368), (15, 0.036...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.4917</td>\n",
       "      <td>[(5, 0.055641364), (8, 0.034521755), (11, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.3335</td>\n",
       "      <td>[(4, 0.051082034), (7, 0.17797029), (10, 0.182...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.3927</td>\n",
       "      <td>[(2, 0.015176013), (4, 0.14343473), (6, 0.2628...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   문서 번호  가장 비중이 높은 토픽  가장 높은 토픽의 비중  \\\n",
       "0      0          14.0        0.4867   \n",
       "1      1          18.0        0.3923   \n",
       "2      2          14.0        0.4417   \n",
       "3      3          19.0        0.3262   \n",
       "4      4           0.0        0.4326   \n",
       "5      5           3.0        0.5310   \n",
       "6      6          10.0        0.3815   \n",
       "7      7          18.0        0.4917   \n",
       "8      8          18.0        0.3335   \n",
       "9      9          18.0        0.3927   \n",
       "\n",
       "                                            각 토픽의 비중  \n",
       "0  [(2, 0.01797956), (4, 0.09671218), (14, 0.4867...  \n",
       "1  [(3, 0.08424776), (11, 0.1929529), (12, 0.0209...  \n",
       "2  [(4, 0.051490474), (5, 0.023157403), (6, 0.017...  \n",
       "3  [(1, 0.04121628), (5, 0.0632835), (6, 0.269845...  \n",
       "4  [(0, 0.43258402), (6, 0.26110706), (18, 0.2779...  \n",
       "5  [(2, 0.017816834), (3, 0.53102016), (6, 0.0819...  \n",
       "6  [(6, 0.26749086), (10, 0.38151368), (15, 0.036...  \n",
       "7  [(5, 0.055641364), (8, 0.034521755), (11, 0.01...  \n",
       "8  [(4, 0.051082034), (7, 0.17797029), (10, 0.182...  \n",
       "9  [(2, 0.015176013), (4, 0.14343473), (6, 0.2628...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\n",
    "topictable[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
