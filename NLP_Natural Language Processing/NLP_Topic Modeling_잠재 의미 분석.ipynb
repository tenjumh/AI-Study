{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토픽 모델링(Topic Modeling)\n",
    "- 한국어로 주제라고 하며 텍스트 본문의 숨겨진 의미 구조를 발견하기 위해 사용되는 텍스트 마이닝 기법(문서 집합의 추상적인 주제를 발견)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) 잠재 의미 분석(Latent Semantic Analysis, LSA)\n",
    "- 토픽 모델링이라는 분야에 아이디어를 제공한 알고리즘.\n",
    "- LDA : LSA의 단점을 개선하여 탄생한 알고리즘으로 토픽 모델리에 보다 적합한 알고리즘\n",
    "- BoW에 기반한 DTM이나 TF-IDF는 단어의 빈도 수를 이용한 수치화 방법이기에 단어의 의미 고려하지 못하는 단점.\n",
    "- DTM의 잠재된(latent)의미를 이끌어내는 방법으로 LSA라는 방법 사용\n",
    "- 선형대수학의 특이값 분해9Singular Value Decomposition, SVD)를 이해 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)특이값 분해(Singular Value Decomposition, SVD)\n",
    "-SVD란 A가 m x n 행렬일 때, 다음과 같이 3개의 행렬의 곱으로 분해하는 것\n",
    "![유클리드거리](https://github.com/tenjumh/GraduateSchool/blob/master/Study/Linear%20Algebra_%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98%ED%95%99/image/1.%20%EC%BA%A1%EC%B2%98.PNG?raw=True)\n",
    "-직교행렬이란 자신과 자신의 전치행렬의 곱 또는 이를 반대로 곱한 결과가 단위행렬이 되는 행렬<br>\n",
    "-대각행렬이란 주대각선을 제외한 곳의 원소가 모두 0인 행렬을 의미<br>\n",
    "-SVD로 나온 대각 행렬의 대각 원소의 값을 행렬 A의 특이값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 전치행렬(Transposed Matrix)<br>\n",
    "전치행렬은 원래의 행렬에서 행과 열을 바꾼 행렬<br>\n",
    "즉, 주대각선을 축으로 반사 대칭하여 얻는 행렬<br>\n",
    "M = [[1, 2], [3, 4], [5, 6]]  (3x2행렬)<br>\n",
    "M^T = [[1, 3, 5], [2, 4, 6]]  (2x3행렬)<br>\n",
    "\n",
    "2) 단위행렬(Identity Matrix)<br>\n",
    "주대각선의 원소가 모두 1이며 나머지 원소는 모두 0인 정사각행렬<br>\n",
    "I라고도 함<br>\n",
    "\n",
    "3) 역행렬(Inverse Matrix)<br>\n",
    "행렬 A와 어떤 행렬을 곱했을 때, 결과로 단위 행렬이 나온다면 이를 A의 역행렬이라고 하며 A^-1이라고 표현\n",
    "![역행렬](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/%EC%97%AD%ED%96%89%EB%A0%AC.PNG?raw=True)\n",
    "\n",
    "4) 직교행렬(Orthogonal Matrix)<br>\n",
    "n x n 행렬 A에 대해서 A x A^T = I를 만족하면서 A^T x A = I를 만족하는 행렬 A를 직교행렬이라고 한다.<br>\n",
    "결국, A^-1 = A^T를 만족한다.\n",
    "\n",
    "5) 대각행렬(Diagonal Matrix)<br>\n",
    "주대각선을 제외한 곳의 원소가 모두 0인 행렬, 이때 주대각선 원소를 a라고 표현\n",
    "![대각행렬](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/%EB%8C%80%EA%B0%81%ED%96%89%EB%A0%AC.PNG?raw=True)\n",
    "-m x n행렬일 때, m > n인 경우\n",
    "![대각행렬](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/%EB%8C%80%EA%B0%81%ED%96%89%EB%A0%AC2.PNG?raw=True)\n",
    "-m x n행렬일 때, m < n인 경우\n",
    "![대각행렬](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/%EB%8C%80%EA%B0%81%ED%96%89%EB%A0%AC3.PNG?raw=True)\n",
    "\n",
    "SVD를 통해 나온 대각 행렬\"시그마\"는 추가적인 성질을 가지는데, 대각행렬 \"시그마\"의 주대각원소를 행렬 A의 특이값이라 함, 내림차순으로 정렬되는 특징이 있음\n",
    "예를 들어 특이값 12.4, 9.5, 1.3이 내림차순으로 정렬되어져 있는 모습\n",
    "![대각행렬](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/%EB%8C%80%EA%B0%81%ED%96%89%EB%A0%AC4.PNG?raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 절단된 SVD(Truncated SVD)\n",
    "지금까지 설명한 SVD는 Full SVD이며 LSA 경우 Full SVD에서 나온 3개의 행렬에서 일부 벡터들을 삭제시킨 Truncated SVD(절단된 SVD)를 사용\n",
    "![SVD](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/SVD_full.PNG?raw=True)\n",
    "\n",
    "![Truncated SVD](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/SVD_thin_compact_truncated.PNG?raw=True)\n",
    "절단된 SVD는 대각행렬 \"시그마\"의 대각 원소의 값 중에서 상위값 t개만 남게된다. 절단된 SVD를 수행하면 값의 손실이 일어나므로 기존의 행렬 A를 복구 불가.<br>\n",
    "또한 U행렬과 V행렬의 t열까지만 남기고 t는 우리가 찾고자하는 토픽의 수를 반영한 하이퍼파라미터값임.<br>\n",
    "하이퍼파라미터란 사용자가 직접 값을 선택하며 선능에 영향을 주는 매개변수로 t를 선택하는 것은 쉽지 않음<br>\n",
    "주대각행렬(\"시그마\")는 내림차순으로 구성되어 있기 때문에 밑으로 갈수록 중요도가 낮아짐 따라서 t를 크게 잡으면 다양한 의미를 포함하며 t를 작게 잡으면 노이즈를 제거<br>\n",
    "일부 벡터를 삭제하는 것은<br> (1)데이터의 차원을 줄인다고도 하는데 계산량도 낮아지는 효과<br> (2)중요하지 않은 정보 삭제 효과<br> (3)즉 설명력이 높은 정보만 남음<br> --> 기존 행렬에서는 드러나지 않았던 심층적인 의미 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) 잠재 의미 분석(Latent Semantic Analysis, LSA)\n",
    "- 기존 DTM이나 DTM에 단어의 중요도에 따른 가중치를 주었던 TF-IDF행렬은 단어의 의미를 전혀 고려하지 못한다는 단점을 갖고 있었다.\n",
    "- LSA는 기본적으로 DTM이나 TF-IDF행렬에 절단된 SVD를 사용하여 차원을 축소, 단어들의 잠재적인 의미를 끌어낸다는 아이디어임\n",
    "![대각행렬](https://github.com/tenjumh/GraduateSchool/blob/master/Study/NLP_Natural%20Language%20Processing/image/LSA%20%EC%98%88%EC%8B%9C.PNG?raw=True)\n",
    "\n",
    "위와 같은 DTM을 실제로 파이썬을 통해 만들면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[0,0,0,1,0,1,1,0,0],[0,0,0,1,1,0,1,0,0],[0,1,1,0,2,0,0,0,0],[1,0,0,0,0,0,0,1,1]])\n",
    "np.shape(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-4 x 9 DMT생성<br>\n",
    "-Full SVD를 수행<br>\n",
    "-\"시그마\"를 S, V^T를 VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, VT = np.linalg.svd(A, full_matrices = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24  0.75  0.   -0.62]\n",
      " [-0.51  0.44 -0.    0.74]\n",
      " [-0.83 -0.49 -0.   -0.27]\n",
      " [-0.   -0.    1.    0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(U.round(2))   #두번째 자리까지만 출력하기위해서 .round(2)를 사용\n",
    "np.shape(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.69 2.05 1.73 0.77]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(s.round(2))\n",
    "np.shape(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Numpy의 linalg.svd()는 특이값 분해의 결과로 대각 행렬이 아니라 특이값의 리스트를 반환한다. 그러므로 다시 대각 행렬로 바꿔 줘야함.<br>\n",
    "우선 특이값을 s에 저장하고 대각행렬 크기의 행렬을 생성한 후 그 행렬에 특이값을 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = np.zeros((4, 9)) # 대각 행렬의 크기인 4 x 9의 임의의 행렬 생성\n",
    "S[:4, :4] = np.diag(s) # 특이값을 대각행렬에 삽입\n",
    "print(S.round(2))\n",
    "np.shape(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대각행렬 S가 내림차순으로 구성된 것을 볼수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]\n",
      " [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]\n",
      " [ 0.   -0.35 -0.35  0.16  0.25 -0.8   0.16 -0.   -0.  ]\n",
      " [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n",
      " [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]\n",
      " [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(VT.round(2))\n",
    "np.shape(VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U x S x VT를 하면 기존 행렬 A가 나와야 함.<br>\n",
    "Numpy의 allclose()는 2개의 행렬이 동일하면 True를 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(A, np.dot(np.dot(U,S),VT).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated SVD\n",
    "- t=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.69 0.  ]\n",
      " [0.   2.05]]\n"
     ]
    }
   ],
   "source": [
    "S=S[:2,:2]   #기존 4 x 9  대각행렬 중 2 x 2만 취함\n",
    "print(S.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24  0.75]\n",
      " [-0.51  0.44]\n",
      " [-0.83 -0.49]\n",
      " [-0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# 직교 행렬 U에 대해서도 2개의 열만 남기고 제거\n",
    "U=U[:,:2]\n",
    "print(U.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# 행렬 V의 전치 행렬인 VT에 대해서 2개의 행만 남김\n",
    "VT=VT[:2,:]\n",
    "print(VT.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "축소된 행렬 U, S, VT에 대해서 U x S x VT연산을 하면 기존 A와 다른 결과가 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 1 0 0]\n",
      " [0 1 1 0 2 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1]]\n",
      "[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]\n",
      " [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]\n",
      " [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.   -0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "A_prime=np.dot(np.dot(U,S), VT)\n",
    "print(A)\n",
    "print(A_prime.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대체적으로 기존에 0인 값들은 0에 가까운 값<br>\n",
    "1인 값들은 1에 가까운 값이 나옴<br>\n",
    "또한 값이 제대로 복구되지 않은 구간도 존재함.<br>\n",
    "축소된 U, S, VT의 크기가 어떤 의미인지 확인\n",
    "\n",
    "-축소된 U는 4 x 2크기를 가지는데 --> 문서의 개수 x 토픽의 수 t의 크기임.<br>\n",
    "-단어의 개수인 9는 유지되지 않는데 문서의 개수인 4의 크기가 유지되었으니 4개의 문서 각각을 2개의 값으로 표현하고 있음<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습\n",
    "- 사이킷런에서 Twenty Newsgroups라고 불리는 20개의 다른 주제를 가진 뉴스 데이터 제공\n",
    "- LSA를 사용 문서의 수를 원하는 토픽의 수로 압축한 뒤에 각 토픽당 가장 중요한 단어 5개를 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 뉴스 데이터에 대한 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers','footeers','quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can\\'t pity you, Jim.  And I\\'m sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won\\'t be bummin\\' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don\\'t forget your Flintstone\\'s Chewables!  :) \\n--\\nBake Timmons, III\\n\\n-- \"...there\\'s nothing higher, stronger, more wholesome and more useful in life\\nthan some good memory...\" -- Alyosha in Brothers Karamazov (Dostoevsky)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first News\n",
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# 사이킷런에서 제공하는 이 뉴스 데이터가 어떤 20개의 카테고리를 갖는지 확인가능\n",
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 텍스트 전처리\n",
    "- 알파벳을 제외한 구두점, 숫자, 특수문자 제거\n",
    "- 짧은 단어 제거\n",
    "- 모든 문자 -> 소문자 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\",\" \")\n",
    "# 길이가 3이하인 단어 제거\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons there nothing higher stronger more wholesome more useful life than some good memory alyosha brothers karamazov dostoevsky'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 불용어 제거 (먼저 토큰화 진행 후 불용어 제거)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# NLTK로부터 불용어 받아옴\n",
    "stop_words = stopwords.words('english')\n",
    "# 토큰화\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "# 불용어 제거\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yeah', 'expect', 'people', 'read', 'actually', 'accept', 'hard', 'atheism', 'need', 'little', 'leap', 'faith', 'jimmy', 'logic', 'runs', 'steam', 'sorry', 'pity', 'sorry', 'feelings', 'denial', 'faith', 'need', 'well', 'pretend', 'happily', 'ever', 'anyway', 'maybe', 'start', 'newsgroup', 'atheist', 'hard', 'bummin', 'much', 'forget', 'flintstone', 'chewables', 'bake', 'timmons', 'nothing', 'higher', 'stronger', 'wholesome', 'useful', 'life', 'good', 'memory', 'alyosha', 'brothers', 'karamazov', 'dostoevsky']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_doc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 불용어인 your, about, just, that, will, after 등이 사라지고 토큰화 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) TF-IDF 행렬 만들기\n",
    "- 불용어 제거를 위해 토큰화를 수행했지만\n",
    "- TfidVectorizer(TF-IDF참고)는 기본적으로 토큰화가 되어있지 않은 텍스트 데이터를 입력으로 사용\n",
    "- 토근화 작업을 역으로 취소하는 작업 수행 (역토큰화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역토큰화 (토큰화 작업을 역으로 되돌림)\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons nothing higher stronger wholesome useful life good memory alyosha brothers karamazov dostoevsky'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1000개에 대한 TF-IDF행렬 수행\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                           max_features = 1000,\n",
    "                           max_df = 0.5,\n",
    "                           smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) 토픽 모델링(Topic Modeling)\n",
    "- TF-IDF 행렬을 다수의 행렬로 분해\n",
    "- 사이킷런의 \"절단된 SVD(Truncated SVD) 사용 : 차원 축수\n",
    "- 기존 뉴스 데이터가 20개의 카테고리를 갖고 있었기에 이를 가정하에 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "svd_model.fit(X)\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- svd_model.components_는 LSA에서 VT에 해당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import numpy as np\n",
    "np.shape(svd_model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 20 X 1000 : 토픽의 수(t) X 단어의 수의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('like', 0.20505), ('know', 0.18838), ('people', 0.18376), ('think', 0.16767), ('good', 0.14274)]\n",
      "Topic 2: [('thanks', 0.3379), ('windows', 0.27465), ('mail', 0.17725), ('card', 0.17113), ('drive', 0.15578)]\n",
      "Topic 3: [('game', 0.38223), ('team', 0.32242), ('year', 0.27387), ('games', 0.24544), ('season', 0.18665)]\n",
      "Topic 4: [('drive', 0.51326), ('scsi', 0.20344), ('disk', 0.15638), ('hard', 0.15618), ('card', 0.15153)]\n",
      "Topic 5: [('thanks', 0.37204), ('drive', 0.3638), ('know', 0.25132), ('scsi', 0.13857), ('advance', 0.12312)]\n",
      "Topic 6: [('windows', 0.34853), ('know', 0.23487), ('like', 0.1898), ('think', 0.17901), ('file', 0.12958)]\n",
      "Topic 7: [('like', 0.55178), ('bike', 0.1782), ('know', 0.17522), ('chip', 0.11768), ('sounds', 0.079)]\n",
      "Topic 8: [('know', 0.24374), ('thanks', 0.22401), ('government', 0.21558), ('people', 0.18357), ('israel', 0.12575)]\n",
      "Topic 9: [('card', 0.51616), ('video', 0.2482), ('monitor', 0.15725), ('sale', 0.15), ('drivers', 0.13072)]\n",
      "Topic 10: [('like', 0.47037), ('armenian', 0.1927), ('people', 0.18913), ('windows', 0.17753), ('turkish', 0.17508)]\n",
      "Topic 11: [('think', 0.46471), ('like', 0.22862), ('internet', 0.18748), ('university', 0.1842), ('people', 0.18006)]\n",
      "Topic 12: [('know', 0.33508), ('space', 0.28752), ('card', 0.2564), ('university', 0.23586), ('nasa', 0.1955)]\n",
      "Topic 13: [('think', 0.47083), ('space', 0.18333), ('bike', 0.15004), ('good', 0.13955), ('year', 0.13732)]\n",
      "Topic 14: [('think', 0.35417), ('space', 0.32648), ('thanks', 0.20217), ('nasa', 0.19711), ('card', 0.11665)]\n",
      "Topic 15: [('know', 0.43391), ('people', 0.2722), ('space', 0.22016), ('file', 0.15128), ('windows', 0.13644)]\n",
      "Topic 16: [('israel', 0.50303), ('israeli', 0.24153), ('know', 0.23028), ('think', 0.18061), ('jews', 0.16094)]\n",
      "Topic 17: [('mail', 0.4988), ('time', 0.41576), ('know', 0.14842), ('think', 0.11952), ('address', 0.11369)]\n",
      "Topic 18: [('good', 0.39667), ('armenian', 0.17696), ('turkish', 0.16658), ('time', 0.16644), ('armenians', 0.14796)]\n",
      "Topic 19: [('think', 0.26404), ('know', 0.24732), ('work', 0.22683), ('email', 0.19504), ('armenian', 0.17921)]\n",
      "Topic 20: [('bike', 0.32458), ('windows', 0.23663), ('right', 0.21794), ('jesus', 0.1686), ('file', 0.15969)]\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()  #단어 집합. 1000개의 단어가 저장됨\n",
    "\n",
    "def get_topics(components, feature_names, n =5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" %(idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(svd_model.components_, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
