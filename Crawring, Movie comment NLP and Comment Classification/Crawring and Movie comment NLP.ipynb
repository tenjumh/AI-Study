{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\anaconda3\\envs\\mytest\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: JPype1>=0.5.7 in c:\\anaconda3\\envs\\mytest\\lib\\site-packages (from konlpy) (0.7.0)\n",
      "Requirement already satisfied: bs4 in c:\\anaconda3\\envs\\mytest\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\anaconda3\\envs\\mytest\\lib\\site-packages (from bs4) (4.8.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\anaconda3\\envs\\mytest\\lib\\site-packages (from beautifulsoup4->bs4) (1.9.5)\n",
      "요청 횟수: 1\n",
      "요청 횟수: 2\n",
      "요청 횟수: 3\n",
      "요청 횟수: 4\n",
      "요청 횟수: 5\n",
      "요청 횟수: 6\n",
      "요청 횟수: 7\n",
      "요청 횟수: 8\n",
      "요청 횟수: 9\n",
      "요청 횟수: 10\n",
      "요청 횟수: 11\n",
      "요청 횟수: 12\n",
      "요청 횟수: 13\n",
      "요청 횟수: 14\n",
      "요청 횟수: 15\n",
      "요청 횟수: 16\n",
      "요청 횟수: 17\n",
      "요청 횟수: 18\n",
      "요청 횟수: 19\n",
      "요청 횟수: 20\n",
      "요청 횟수: 21\n",
      "요청 횟수: 22\n",
      "요청 횟수: 23\n",
      "요청 횟수: 24\n",
      "요청 횟수: 25\n",
      "요청 횟수: 26\n",
      "요청 횟수: 27\n",
      "요청 횟수: 28\n",
      "요청 횟수: 29\n",
      "요청 횟수: 30\n",
      "요청 횟수: 31\n",
      "요청 횟수: 32\n",
      "요청 횟수: 33\n",
      "요청 횟수: 34\n",
      "요청 횟수: 35\n",
      "요청 횟수: 36\n",
      "요청 횟수: 37\n",
      "요청 횟수: 38\n",
      "요청 횟수: 39\n",
      "요청 횟수: 40\n",
      "요청 횟수: 41\n",
      "요청 횟수: 42\n",
      "요청 횟수: 43\n",
      "요청 횟수: 44\n",
      "요청 횟수: 45\n",
      "요청 횟수: 46\n",
      "요청 횟수: 47\n",
      "요청 횟수: 48\n",
      "요청 횟수: 49\n",
      "요청 횟수: 50\n",
      "요청 횟수: 51\n",
      "요청 횟수: 52\n",
      "요청 횟수: 53\n",
      "요청 횟수: 54\n",
      "요청 횟수: 55\n",
      "요청 횟수: 56\n",
      "요청 횟수: 57\n",
      "요청 횟수: 58\n",
      "요청 횟수: 59\n",
      "요청 횟수: 60\n",
      "요청 횟수: 61\n",
      "요청 횟수: 62\n",
      "요청 횟수: 63\n",
      "요청 횟수: 64\n",
      "요청 횟수: 65\n",
      "요청 횟수: 66\n",
      "요청 횟수: 67\n",
      "요청 횟수: 68\n",
      "요청 횟수: 69\n",
      "요청 횟수: 70\n",
      "요청 횟수: 71\n",
      "요청 횟수: 72\n",
      "요청 횟수: 73\n",
      "요청 횟수: 74\n",
      "요청 횟수: 75\n",
      "요청 횟수: 76\n",
      "요청 횟수: 77\n",
      "요청 횟수: 78\n",
      "요청 횟수: 79\n",
      "요청 횟수: 80\n",
      "요청 횟수: 81\n",
      "요청 횟수: 82\n",
      "요청 횟수: 83\n",
      "요청 횟수: 84\n",
      "요청 횟수: 85\n",
      "요청 횟수: 86\n",
      "요청 횟수: 87\n",
      "요청 횟수: 88\n",
      "요청 횟수: 89\n",
      "요청 횟수: 90\n",
      "요청 횟수: 91\n",
      "요청 횟수: 92\n",
      "요청 횟수: 93\n",
      "요청 횟수: 94\n",
      "요청 횟수: 95\n",
      "요청 횟수: 96\n",
      "요청 횟수: 97\n",
      "요청 횟수: 98\n",
      "요청 횟수: 99\n",
      "요청 횟수: 100\n",
      "요청 횟수: 101\n",
      "요청 횟수: 102\n",
      "요청 횟수: 103\n",
      "요청 횟수: 104\n",
      "요청 횟수: 105\n",
      "요청 횟수: 106\n",
      "요청 횟수: 107\n",
      "요청 횟수: 108\n",
      "요청 횟수: 109\n",
      "요청 횟수: 110\n",
      "요청 횟수: 111\n",
      "요청 횟수: 112\n",
      "요청 횟수: 113\n",
      "요청 횟수: 114\n",
      "요청 횟수: 115\n",
      "요청 횟수: 116\n",
      "요청 횟수: 117\n",
      "요청 횟수: 118\n",
      "요청 횟수: 119\n",
      "요청 횟수: 120\n",
      "요청 횟수: 121\n",
      "요청 횟수: 122\n",
      "요청 횟수: 123\n",
      "요청 횟수: 124\n",
      "요청 횟수: 125\n",
      "요청 횟수: 126\n",
      "요청 횟수: 127\n",
      "요청 횟수: 128\n",
      "요청 횟수: 129\n",
      "요청 횟수: 130\n",
      "요청 횟수: 131\n",
      "요청 횟수: 132\n",
      "요청 횟수: 133\n",
      "요청 횟수: 134\n",
      "요청 횟수: 135\n",
      "요청 횟수: 136\n",
      "요청 횟수: 137\n",
      "요청 횟수: 138\n",
      "요청 횟수: 139\n",
      "요청 횟수: 140\n",
      "요청 횟수: 141\n",
      "요청 횟수: 142\n",
      "요청 횟수: 143\n",
      "요청 횟수: 144\n",
      "요청 횟수: 145\n",
      "요청 횟수: 146\n",
      "요청 횟수: 147\n",
      "요청 횟수: 148\n",
      "요청 횟수: 149\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy\n",
    "!pip install bs4\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "################    크롤링  #################\n",
    "\n",
    "twitter = Okt()\n",
    "url = \"https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?\" \\\n",
    "      \"code=181414&type=after&isActualPointWriteExecute=false&\" \\\n",
    "      \"isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page={}\"\n",
    "\n",
    "\n",
    "# 클리닝 함수\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub('[a-zA-Z]', '', text)\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "                          '', cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "def get_reple(page=1):\n",
    "    response = requests.get(url.format(page))\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    s, t = [], []\n",
    "\n",
    "    for li in soup.find('div', {'class': 'score_result'}).find_all('li'):\n",
    "        # text를 가져올때 공백을 제거한다\n",
    "        str = li.p.get_text(\" \", strip=True)\n",
    "        str = clean_text(str)\n",
    "\n",
    "        # 8점이상 좋은영화\n",
    "        if int(li.em.text) >= 8:\n",
    "            s.append(1)\n",
    "            t.append(str)\n",
    "        # 5점 이하 좋지 않은영화\n",
    "        elif int(li.em.text) <= 5:\n",
    "            s.append(0)\n",
    "            t.append(str)\n",
    "    return s, t\n",
    "\n",
    "\n",
    "score, text = [], []\n",
    "\n",
    "# 1~150page까지 불러온 후 배열에 넣어준다.\n",
    "for i in range(1, 150):\n",
    "    print('요청 횟수:', i)\n",
    "    s, t = get_reple(i)\n",
    "    # 배열을 합하여준다.\n",
    "    score += s\n",
    "    text += t\n",
    "\n",
    "df = pd.DataFrame([score, text]).T\n",
    "df.columns = ['score', 'text']\n",
    "# csv파일로 저장한다.\n",
    "df.to_csv('test.csv', encoding='utf-8-sig')\n",
    "\n",
    "################    여기까지 크롤링  #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "758     개연성이 떨어지고 스토리 완성도도 떨어진다  너무 스토리가 엉망이라 판타지인줄 알았...\n",
      "938                                  관람객 긴장감 쩔어서 재밌게 봤습니당\n",
      "10      기대 진짜 많이하고갔는데 큰 의미를 부여하는건 좋았지만 그걸 지루하지않고 쉽게 풀어...\n",
      "991     ㅈ평점왜케낮아졌지 ㅋㅋㅋ 마지막 1분을 위한 영화 또 중간중간에 장치들이 많아서 해...\n",
      "1112    평론가들 너도나도 빨아재끼는 문화적 허영심 역겹다 그래 미국의 모순적인 부분과 상징...\n",
      "                              ...                        \n",
      "763                             다들 연기도 잘하고 재밌었다ㅋㅋ 역시 기대이상\n",
      "835                           재밌었음 스토리도 연출도 배우들 연기가 장난 아님\n",
      "1216                                                어스겟아웃\n",
      "559            믿을수없는 평점임 2점 주는것도 아까움 도플갱어가 입여는순간부터 망한영화느낌\n",
      "684                                    넘나 지루하고 내용을 잘모르겠네요\n",
      "Name: text, Length: 1016, dtype: object\n",
      "758     0.0\n",
      "938     1.0\n",
      "10      0.0\n",
      "991     1.0\n",
      "1112    0.0\n",
      "       ... \n",
      "763     1.0\n",
      "835     1.0\n",
      "1216    1.0\n",
      "559     0.0\n",
      "684     0.0\n",
      "Name: score, Length: 1016, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "###############     LogisticRegression을 이용한 감정분석 with TF-IDF,BOW(TfidfVectorizer사용)    #################\n",
    "# TF-IDF란? https://nesoy.github.io/articles/2017-11/tf-idf\n",
    "# BOW란? url1 :: https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-7%EC%9D%BC%EC%B0%A8-bag-of-words-20b2af01d56f\n",
    "#        url2 :: https://ldabook.com/word-representations.html\n",
    "# TfidfVectorizer란? https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/\n",
    "# df = pd.read_csv('test.csv', encoding='utf-8')\n",
    "# x값: bow로 벡터화한값, y: 영화 평점\n",
    "text = df.get('text')\n",
    "score = df.get('score')\n",
    "\n",
    "# 트레이닝/테스트셋을 분리한다.\n",
    "# random_state: random으로 나누어준다, 0은 항상 같은결과가 나오도록 seed고정\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(text, score, test_size=0.2, random_state=0)\n",
    "train_y = train_y.astype('float')\n",
    "test_y = test_y.astype('float')\n",
    "type(train_x)\n",
    "print(train_x)\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1016x1472 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14536 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어들을 모아 빈도수 체크 (TF-IDF)\n",
    "# feature(문장의 특징) 노출수를 가중치로 설정한 Bag Of Word 벡터를 만든다.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ngram으로 단어를 n개로 잘라서 보겠다는 의미 (1, 2그램 사용했다)\n",
    "# ex) 재밌다. 너무 재밌다. 노잼. 완전 노잼.\n",
    "# tokenizer는 twitter.morphs사용\n",
    "# max_df: 너무 많은단어들(the, a)제외\n",
    "# min_df: 너무적은단어 제외 몇만개에서 3개만 나오는 단어의 경우 의미가 없을 가능성이 높다\n",
    "# https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/\n",
    "tfv = TfidfVectorizer(tokenizer=twitter.morphs, ngram_range=(1, 2), min_df=3, max_df=0.9)\n",
    "tfv.fit(train_x)\n",
    "tfv_train_x = tfv.transform(train_x)\n",
    "tfv_train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0  0 점    1  1 도  1 위  1 점  1 점도   10  10 점   15  ...  후반 부  후반 부로  \\\n",
      "0     0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...   0.0    0.0   \n",
      "1     0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...   0.0    0.0   \n",
      "2     0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...   0.0    0.0   \n",
      "3     0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...   0.0    0.0   \n",
      "4     0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...   0.0    0.0   \n",
      "...   ...  ...  ...  ...  ...  ...   ...  ...   ...  ...  ...   ...    ...   \n",
      "1011  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...   0.0    0.0   \n",
      "1012  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...   0.0    0.0   \n",
      "1013  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...   0.0    0.0   \n",
      "1014  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...   0.0    0.0   \n",
      "1015  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...   0.0    0.0   \n",
      "\n",
      "       후회    훨   훨씬    휴   흑인    흠  흥미진진    힘  \n",
      "0     0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "1     0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "2     0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "3     0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "4     0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "...   ...  ...  ...  ...  ...  ...   ...  ...  \n",
      "1011  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "1012  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "1013  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "1014  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "1015  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "\n",
      "[1016 rows x 1472 columns]\n"
     ]
    }
   ],
   "source": [
    "## 참고(없어도 학습가능) tf-idf 형태 ##\n",
    "#  tfidf 값을 가중치로 설정한 행렬을 볼 수 있다.\n",
    "# 부정예시 : 돈 아깝다\n",
    "features = tfv.get_feature_names()\n",
    "stm = np.asarray(tfv_train_x.toarray())\n",
    "df = pd.DataFrame(stm)\n",
    "df.columns = features\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Anaconda3\\envs\\mytest\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=0, solver='warn',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'C': [0.1, 1, 0.01, 3, 5, 10]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이진분류 알고리즘(로지스틱 회귀)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# 하이퍼파라미터 최적화\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "params = {'C': [0.1, 1, 0.01, 3, 5, 10]}\n",
    "\n",
    "# params중 정확도가 가장 높은 param을 선택\n",
    "# cv=4 : 4번의 모의를 통해 가장 좋은 params를 선택\n",
    "# https://datascienceschool.net/view-notebook/ff4b5d491cc34f94aea04baca86fbef8/ 참조\n",
    "# verbose: 상세정보(숫자가 클수록 상세정보가 자세히 보여진다)\n",
    "grid_cv = GridSearchCV(clf, param_grid=params, cv=4, scoring='accuracy', verbose=1)\n",
    "\n",
    "# 최적의 하이퍼파라미터를 사용하여 학습\n",
    "grid_cv.fit(tfv_train_x, train_y)\n",
    "# grid_cv: 학습한 모델에 접근할 수 있도록 PREDICT와 SCORE 메서드 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10}\n",
      "0.8208661417322834\n"
     ]
    }
   ],
   "source": [
    "# 가장 좋은 parameter\n",
    "print(grid_cv.best_params_)\n",
    "# 가장 좋은 parameter\n",
    "print(grid_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7795275590551181"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data 학습\n",
    "\n",
    "# 벡터변환기\n",
    "tfv_test_x = tfv.transform(test_x)\n",
    "# 정확도 예측 (train data와 test data의 정확도를 비교하며 over fitting에 빠졌는지 확인해준다)\n",
    "grid_cv.best_estimator_.score(tfv_test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "# 만들어진 모델로 영화 긍부정 구분\n",
    "\n",
    "# 긍정 테스트\n",
    "#ex = ['진짜 재밌어요! 강추!!']\n",
    "\n",
    "# 부정 테스트\n",
    "ex = ['좀 그랬어요']\n",
    "# 감정분석사전과는 달리 비속어도 구분이 가능하다\n",
    "# 하지만 라벨링이 되어있지않은 소셜데이터의경우 감정분석사전을 사용하는게 더 좋을 수 있다.\n",
    "my_review = tfv.transform(ex)\n",
    "\n",
    "# 0: 부정, 1: 긍정\n",
    "if grid_cv.best_estimator_.predict(my_review) == 0:\n",
    "    print(\"부정\")\n",
    "else:\n",
    "    print(\"긍정\")\n",
    "print(grid_cv.best_estimator_.predict(my_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0  0 점    1  1 도  1 위  1 점  1 점도   10  10 점   15  ...  후반 부  후반 부로   후회  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...   0.0    0.0  0.0   \n",
      "\n",
      "     훨   훨씬    휴   흑인    흠  흥미진진    힘  \n",
      "0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "\n",
      "[1 rows x 1472 columns]\n"
     ]
    }
   ],
   "source": [
    "## 참고 (없어도 학습가능) 입력 tf-idf 형태 ##\n",
    "features = tfv.get_feature_names()\n",
    "stm = np.asarray(my_review.toarray())\n",
    "df = pd.DataFrame(stm)\n",
    "df.columns = features\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
